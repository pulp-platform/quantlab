{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c91f862",
   "metadata": {},
   "source": [
    "# Quantising a MobileNetV2 with the PACT algorithm\n",
    "\n",
    "This notebook shows how how to create an integerised [MobileNetV2](https://ieeexplore.ieee.org/document/8578572) using the QuantLib package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b866f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, List, Union, Optional, Type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8065fc",
   "metadata": {},
   "source": [
    "## Part 1: creating and evaluating a floating-point network\n",
    "\n",
    "### Step 1: check the computing infrastructure\n",
    "\n",
    "Depending on the hardware at our disposal, we will make different training and testing processes choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf78ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available system CPU(s): 20.\n",
      "Available system GPU(s): 2.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "print(f\"Available system CPU(s): {n_cpus}.\")\n",
    "print(f\"Available system GPU(s): {n_gpus}.\")\n",
    "\n",
    "device = torch.device(torch.cuda.current_device()) if (n_gpus > 0) else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314107e5",
   "metadata": {},
   "source": [
    "### Step 2: load data into PyTorch\n",
    "\n",
    "Every problem in supervised learning requires a data set.\n",
    "We can partition the data points of this data set into three categories:\n",
    "* **training** points: they are available at training time and we can use them to update the model's parameters;\n",
    "* **validation** points: they are available at training time, we can not use them to update the parameters, but we can use them to measure the performance of the model throughout the learning process;\n",
    "* **test** points: they are not available at training time but only once the model is deployed in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ebacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto, unique\n",
    "\n",
    "\n",
    "# each supervised learning problem defines a training/validation/test partition of its data points\n",
    "@unique\n",
    "class DataSetPartition(Enum):\n",
    "    TRAINING   = auto()\n",
    "    VALIDATION = auto()\n",
    "    TEST       = auto()\n",
    "\n",
    "    @classmethod\n",
    "    def canonicalise(cls, partition_spec: DataSetPartitionSpec) -> DataSetPartition:\n",
    "\n",
    "        # validate specification type\n",
    "        if not isinstance(partition_spec, (DataSetPartition, str,)):\n",
    "            raise TypeError\n",
    "\n",
    "        if isinstance(partition_spec, DataSetPartition):\n",
    "            partition = partition_spec\n",
    "        \n",
    "        else:  # `isinstance(partition, str)`\n",
    "\n",
    "            partition_spec = partition_spec.upper()\n",
    "            if partition_spec in {'TRAINING', 'TRAIN'}:\n",
    "                partition = cls['TRAINING']\n",
    "            elif partition_spec in {'VALIDATION', 'VALID'}:\n",
    "                partition = cls['VALIDATION']\n",
    "            elif partition_spec in {'TEST', 'TESTING'}:\n",
    "                partition = cls['TEST']\n",
    "            else:\n",
    "                raise ValueError\n",
    "        \n",
    "        return partition\n",
    "\n",
    "    \n",
    "# define the ways in which a user can specify data set partitions\n",
    "DataSetPartitionSpec = Union[DataSetPartition, str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dce90c",
   "metadata": {},
   "source": [
    "PyTorch represents data points using the `torch.Tensor` data structure.\n",
    "PyTorch uses a specific pipeline to transform files stored on disk into mini-batches of `Tensor`s.\n",
    "This pipeline consists of four stages.\n",
    "* Define a function to transform a Python object into a corresponding collection of `Tensor`s. For instance, we can map a labelled image object to a pair consisting of an input tensor and a label tensor. These functions are called *transforms* and can be composed to describe complex pre-processing transformations (see `torchvision.transforms.Compose`).\n",
    "* Define a `torch.utils.data.Dataset` to index the files stored on disk, specify how to load each file into a Python object, and specify how to map such object to a corresponding collection of `Tensor`s.\n",
    "* Define a `torch.utils.data.Sampler` that can sample and return mini-batches from a given list of integers. This batching can be performed with or without permuting the list. If the list is permuted, it can be permuted once (without repetition) or in-between each sampling (with repetition).\n",
    "* Define a `torch.utils.data.DataLoader` specifying the mini-batches size and the number of worker threads that should be used to load data point files from disk. `DataLoader`s work as follow:\n",
    "  * the `DataLoader` queries the `Sampler` for a list of indices;\n",
    "  * the `DataLoader` distributes these indices to the worker threads;\n",
    "  * each thread applies the `transforms` specified by the `Dataset` to the files corresponding to the received indices; since `transforms` should be applied individually to each data point, `DataLoader`s apply a *map pattern* to parallelise the work amongst multiple workers;\n",
    "  * each thread returns the (loaded and pre-processed) `Tensor` data points to the `DataLoader`;\n",
    "  * the `DataLoader` collates these objects into a mini-batch of `Tensor`s, and returns these mini-batches.\n",
    "\n",
    "Let's start by defining ImageNet-specific transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5b8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomHorizontalFlip, RandomResizedCrop\n",
    "from torchvision.transforms import Resize, CenterCrop\n",
    "from torchvision.transforms import ToTensor, Normalize, Lambda\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "ImageNetStats = \\\n",
    "    {\n",
    "        'normalise':\n",
    "            {\n",
    "                'mean': (0.485, 0.456, 0.406),\n",
    "                'std':  (0.229, 0.224, 0.225)\n",
    "            },\n",
    "        'quantise':\n",
    "            {\n",
    "                'min':   -2.1179039478,  # computed on the normalised images of the validation partition\n",
    "                'max':   2.6400001049,   # computed on the normalised images of the validation partition\n",
    "                'scale': 0.020625000819563866\n",
    "            }\n",
    "    }\n",
    "\n",
    "\n",
    "class ImageNetNormalise(Normalize):\n",
    "    def __init__(self):\n",
    "        super(ImageNetNormalise, self).__init__(**ImageNetStats['normalise'])\n",
    "        \n",
    "\n",
    "class ImageNetIntegerise(Lambda):\n",
    "    def __init__(self):\n",
    "        INT8_MIN = -2**(8-1)\n",
    "        INT8_MAX = 2**(8-1) - 1\n",
    "        image_scale = ImageNetStats['quantise']['scale']\n",
    "        super(ImageNetIntegerise, self).__init__(lambda x: torch.clip((x / image_scale).floor(), INT8_MIN, INT8_MAX))\n",
    "\n",
    "\n",
    "class ImageNetTransform(Compose):\n",
    "\n",
    "    def __init__(self, partition_spec: DataSetPartitionSpec, image_size: int = 224, integerise: bool = False):\n",
    "\n",
    "        # validate arguments\n",
    "        RESIZE_SIZE = 256\n",
    "        if not (image_size <= RESIZE_SIZE):\n",
    "            raise ValueError  # otherwise, we can not crop the resized image to the desired size\n",
    "\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "\n",
    "        if partition is DataSetPartition['TRAINING']:\n",
    "            transforms = [RandomHorizontalFlip(),\n",
    "                          RandomResizedCrop(image_size)]\n",
    "        else:\n",
    "            transforms = [Resize(RESIZE_SIZE),\n",
    "                          CenterCrop(image_size)]\n",
    "\n",
    "        transforms += [ToTensor(),\n",
    "                       ImageNetNormalise(),\n",
    "                       ImageNetIntegerise()]\n",
    "        \n",
    "        if not integerise:\n",
    "            transforms += [Lambda(lambda x: x * ImageNetStats['quantise']['scale'])]  # return a fake-quantised `Tensor`\n",
    "\n",
    "        super(ImageNetTransform, self).__init__(transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca7a35",
   "metadata": {},
   "source": [
    "We can use a [factory pattern](https://en.wikipedia.org/wiki/Factory_method_pattern) to create training and validation `DataLoader`s while reducing code duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687e77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ImageNetDataLoaderFactory(object):\n",
    "    \n",
    "    def __init__(self, path_data: str):\n",
    "        \n",
    "        if not os.path.isdir(path_data):\n",
    "            raise FileNotFounderror  # missing ImageNet data folder\n",
    "\n",
    "        super(ImageNetDataLoaderFactory, self).__init__()\n",
    "\n",
    "        self._partition_to_subfolder = OrderedDict([\n",
    "            (DataSetPartition['TRAINING'], os.path.join(path_data, 'train')),\n",
    "            (DataSetPartition['VALIDATION'], os.path.join(path_data, 'val')),\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def get_dataset(self,\n",
    "                    partition_spec: DataSetPartitionSpec,\n",
    "                    transform:      torchvision.transforms.Compose) -> torch.utils.data.Dataset:\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "        return torchvision.datasets.ImageFolder(self._partition_to_subfolder[partition], transform)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_sampler(partition_spec: DataSetPartitionSpec,\n",
    "                    dataset: torch.utils.data.Dataset) -> torch.utils.data.Sampler:\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "        return torch.utils.data.RandomSampler(dataset) if (partition is DataSetPartition['TRAINING']) else torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    def get_dataloader(self,\n",
    "                       partition_spec: DataSetPartitionSpec,\n",
    "                       transform:      torchvision.transforms.Compose,\n",
    "                       batch_size:     int,\n",
    "                       num_workers:    int = 1) -> torch.utils.data.DataLoader:\n",
    "\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "\n",
    "        dataset = self.get_dataset(partition, transform)\n",
    "        sampler = ImageNetDataLoaderFactory.get_sampler(partition, dataset)\n",
    "        loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                             sampler=sampler,\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=num_workers)\n",
    "\n",
    "        return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac188a",
   "metadata": {},
   "source": [
    "We are now ready to create our training and validation `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc13a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the `DataLoader` factory\n",
    "path_data = os.path.join(os.curdir, 'data')\n",
    "loader_factory = ImageNetDataLoaderFactory(path_data)\n",
    "\n",
    "per_gpu_batch_size = 48\n",
    "batch_size = max(1, n_gpus) * per_gpu_batch_size  # `nn.DataParallel` will revert this dispatching `per_gpu_batch_size` items to each GPU\n",
    "\n",
    "# create the training `DataLoader`\n",
    "train_transform = ImageNetTransform('train')\n",
    "train_loader = loader_factory.get_dataloader('train', train_transform, batch_size, num_workers=n_cpus)\n",
    "\n",
    "# create the validation `DataLoader`\n",
    "valid_transform = ImageNetTransform('valid')\n",
    "valid_loader = loader_factory.get_dataloader('valid', valid_transform, batch_size, num_workers=n_cpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412f36c",
   "metadata": {},
   "source": [
    "### Step 3: create a floating-point PyTorch network\n",
    "\n",
    "PyTorch represents deep neural networks as `torch.nn.Module`s.\n",
    "\n",
    "Since deep neural networks can be (and are often) modelled as function compositions, `Module`s have been designed to be composed to create complex functions, i.e., complex networks.\n",
    "For this reason, we can distinguish between:\n",
    "* *atomic* `Module`s; examples are linear operations, batch normalisations, and activation operations;\n",
    "* *container* `Module`s; containers are used to express layers (usually concatenating linear and activation operations, with possibly a batch normalisation in-between) and blocks of layers.\n",
    "\n",
    "Differently from the MobileNetV1 architecture and similarly to [ResNets](https://ieeexplore.ieee.org/document/7780459), MobileNetV2 has a branching topology: it uses blocks modelling a function as the sum of the identity (*skip branch*) and a non-identity (*residual branch*) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630b4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "CONFIGS = OrderedDict([\n",
    "    ('STANDARD', [\n",
    "        # t,  c, n, s\n",
    "        [1,  16, 1, 1],\n",
    "        [6,  24, 2, 2],\n",
    "        [6,  32, 3, 2],\n",
    "        [6,  64, 4, 2],\n",
    "        [6,  96, 3, 1],\n",
    "        [6, 160, 3, 2],\n",
    "        [6, 320, 1, 1]\n",
    "    ]),\n",
    "])\n",
    "\n",
    "\n",
    "ACTIVATIONS = ('relu', 'relu6',)\n",
    "\n",
    "\n",
    "class Conv2dBN2dActivation(nn.Sequential):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels:        int,\n",
    "                 out_channels:       int,\n",
    "                 kernel_size:        int = 3,\n",
    "                 stride:             int = 1,\n",
    "                 groups:             int = 1,\n",
    "                 activation_class:   Type[nn.Module] = nn.ReLU6):\n",
    "\n",
    "        modules = []\n",
    "        modules += [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2, groups=groups, bias=False)]\n",
    "        modules += [nn.BatchNorm2d(out_channels)]\n",
    "        modules += [activation_class(inplace=True)]  # we assume that the `activation_class` is \"inplaceable\"\n",
    "\n",
    "        super(Conv2dBN2dActivation, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class Conv2dBN2d(nn.Sequential):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels:  int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size:  int = 3,\n",
    "                 stride:       int = 1,\n",
    "                 groups:       int = 1):\n",
    "\n",
    "        modules = []\n",
    "        modules += [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2, groups=groups, bias=False)]\n",
    "        modules += [nn.BatchNorm2d(out_channels)]\n",
    "\n",
    "        super(Conv2dBN2d, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_planes:        int,\n",
    "                 out_planes:       int,\n",
    "                 stride:           int,\n",
    "                 expansion_factor: int,\n",
    "                 activation_class: Type[nn.Module] = nn.ReLU6):\n",
    "\n",
    "        # validate `stride` argument\n",
    "        if stride not in {1, 2}:\n",
    "            raise ValueError\n",
    "\n",
    "        super(InvertedResidual, self).__init__()\n",
    "\n",
    "        hid_planes = in_planes * expansion_factor\n",
    "\n",
    "        # build the residual branch\n",
    "        layers = []\n",
    "        # (optional) input layer\n",
    "        if expansion_factor > 1:  # this is an \"inverted\" residual (instead of reducing the number of channels as in ResNets, the input layer increases it)\n",
    "            # point-wise convolution\n",
    "            layers += [Conv2dBN2dActivation(in_planes, hid_planes, kernel_size=1, activation_class=activation_class)]\n",
    "        else:\n",
    "            assert(hid_planes == in_planes)\n",
    "            layers += []\n",
    "        # middle layer (depth-wise convolution)\n",
    "        layers += [Conv2dBN2dActivation(hid_planes, hid_planes, kernel_size=3, stride=stride, groups=hid_planes, activation_class=activation_class)]\n",
    "        # output layer (point-wise convolution)\n",
    "        layers += [Conv2dBN2d(hid_planes, out_planes, kernel_size=1)]\n",
    "        # concatenate input, middle, and output layers\n",
    "        self.residual_branch = nn.Sequential(*layers)\n",
    "\n",
    "        # should I use skip (i.e., identity) branch?\n",
    "        self._use_skip_branch = (stride == 1) and (out_planes == in_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self._use_skip_branch:\n",
    "            return x + self.residual_branch(x)\n",
    "        else:  # this is a bottleneck layer (i.e., free from skip-branches)\n",
    "            return self.residual_branch(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 config:                       str,\n",
    "                 capacity:                     float = 1.0,\n",
    "                 round_to_closest_multiple_of: int = 8,\n",
    "                 activation:                   str = 'ReLU6',\n",
    "                 n_classes:                    int = 1000,\n",
    "                 seed :                        int = -1,\n",
    "                 pretrained : str = None):\n",
    "\n",
    "        # validate inputs\n",
    "        config = config.upper()  # canonicalise\n",
    "        if config not in CONFIGS.keys():\n",
    "            raise ValueError  # invalid configuration\n",
    "            \n",
    "        if not (0.0 < capacity <= 1.0):\n",
    "            raise ValueError  # capacity must be a positive, compressive (i.e., not greater than one) scaling factor\n",
    "\n",
    "        activation = activation.lower()  # canonicalise\n",
    "        if activation not in ACTIVATIONS:\n",
    "            raise ValueError  # invalid activation function\n",
    "        if activation == 'relu':\n",
    "            activation_class = nn.ReLU\n",
    "        else:  # activation == 'relu6':\n",
    "            activation_class = nn.ReLU6\n",
    "\n",
    "        super(MobileNetV2, self).__init__()\n",
    "\n",
    "        # compute structural hyper-parameters of the input and output of the `features` sub-network\n",
    "        out_channels_pilot    = MobileNetV2.make_divisible_by(32 * capacity, divisor=round_to_closest_multiple_of)\n",
    "        in_planes_features    = out_channels_pilot\n",
    "        out_planes_features   = MobileNetV2.make_divisible_by(1280 * capacity, divisor=round_to_closest_multiple_of)\n",
    "        out_channels_features = out_planes_features\n",
    "\n",
    "        # build the network\n",
    "        self.pilot      = MobileNetV2.make_pilot(out_channels_pilot, activation_class)\n",
    "        self.features   = MobileNetV2.make_features(config, capacity, round_to_closest_multiple_of, activation_class, in_planes_features, out_planes_features)\n",
    "        self.avgpool    = MobileNetV2.make_avgpool()\n",
    "        self.classifier = MobileNetV2.make_classifier(out_channels_features, n_classes)\n",
    "\n",
    "        # initialise the parameters\n",
    "        self._initialize_weights(seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_divisible_by(dilated_n_channels: float,\n",
    "                          divisor:            int,\n",
    "                          min_n_channels:     Optional[int] = None) -> int:\n",
    "\n",
    "        # canonicalise the `min_n_channels` argument\n",
    "        if min_n_channels is None:\n",
    "            min_n_channels = divisor\n",
    "        if (min_n_channels % divisor) != 0:\n",
    "            raise ValueError  # `min_n_channels` must be divisible by `divisor`\n",
    "\n",
    "        # compute the required `n_channels`\n",
    "        n_channels = max(min_n_channels, (int(dilated_n_channels + divisor / 2) // divisor) * divisor)\n",
    "        if (dilated_n_channels - n_channels) / dilated_n_channels >= 0.1:\n",
    "            n_channels += divisor\n",
    "\n",
    "        return n_channels\n",
    "\n",
    "    @staticmethod\n",
    "    def make_pilot(out_channels_pilot: int,\n",
    "                   activation_class:   Type[nn.Module]) -> Conv2dBNActivation:\n",
    "        return Conv2dBN2dActivation(3, out_channels_pilot, kernel_size=3, stride=2, activation_class=activation_class)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_features(config:                       str,\n",
    "                      capacity:                     float,\n",
    "                      round_to_closest_multiple_of: int,\n",
    "                      activation_class:             Type[nn.Module],\n",
    "                      in_planes_features:           int,\n",
    "                      out_planes_features:          int) -> nn.Sequential:\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        # create the backbone\n",
    "        in_planes = in_planes_features\n",
    "        for t, c, n, s in CONFIGS[config]:\n",
    "            out_planes = MobileNetV2.make_divisible_by(c * capacity, divisor=round_to_closest_multiple_of)\n",
    "            for block_id in range(0, n):\n",
    "                stride = s if block_id == 0 else 1\n",
    "                blocks += [InvertedResidual(in_planes, out_planes, stride, t, activation_class=activation_class)]\n",
    "                in_planes = out_planes\n",
    "\n",
    "        # add a last layer\n",
    "        blocks += [Conv2dBN2dActivation(in_planes, out_planes_features, kernel_size=1, activation_class=activation_class)]\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_avgpool() -> nn.AdaptiveAvgPool2d:\n",
    "        return nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def make_classifier(out_channels_features: int, n_classes: int) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "        modules += [nn.Dropout(0.2)]\n",
    "        modules += [nn.Linear(out_channels_features, n_classes)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.pilot(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, seed : int = -1) -> None:\n",
    "\n",
    "        if seed >= 0:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db8ff7",
   "metadata": {},
   "source": [
    "We are ready to create our MobileNetV2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13771ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a MobileNetV2\n",
    "config = 'standard'\n",
    "capacity = 1.0\n",
    "activation = 'relu6'  # other option: `relu`\n",
    "\n",
    "mnv2 = MobileNetV2(config=config, capacity=capacity, activation=activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd8689",
   "metadata": {},
   "source": [
    "A fundamental functionality of PyTorch is its convenient interface for using GPUs.\n",
    "Before we explain how this interface works, we need to provide some additional information about the workings of PyTorch `Tensor`s.\n",
    "\n",
    "Each PyTorch `Tensor` is a wrapper abstraction around a *payload* array.\n",
    "Apart from this payload, a `Tensor` has other attributes and methods, most of which operate on the payload (we call these *payload methods*).\n",
    "One of these attributes is `device`, which indicates whether the payload is stored on the main memory of the computing system (i.e., the one managed directly by the CPU) or on the memory of some accelerator attached to the computing system (e.g., a GPU). The C++ backend of `Tensor`s includes several implementations of payload methods, one for each computing device that might be available on our computing system (e.g., CPU vs. GPU).\n",
    "Depending on the `device` attributes of the `Tensor`s involved in a given operation, PyTorch's runtime engine dispatches the code to the correct version of the payload method.\n",
    "\n",
    "Most `Module`s have parameters or hyper-parameters, but moving them individually and manually to the correct device memory can become cumbersome and is error-prone.\n",
    "Thus, PyTorch `Module`s expose a `to` method which can be used to automatically move all the parameters and hyper-parameters to the correct device memory.\n",
    "Due to the hierarchical nature of `Module` compositions, calling `to` on a container `Module` will invoke the same call on its children `Module`s.\n",
    "\n",
    "If more than one GPU is available on our system, it is possible to wrap a `Module` into an `nn.DataParallel` object.\n",
    "At runtime, this object will automatically replicate the `Module` and map a replica to each available GPU; then, it will partition mini-batches to distribute data points evenly amongst all the available GPUs.\n",
    "`DataParallel` objects are a convenient abstraction to exploit all the computational power available on your computing system.\n",
    "\n",
    "However, note that QuantLib's editing functionalities only work on `Module`s.\n",
    "Since `DataParallel` objects are wrappers around `Module`s but not `Module` objects, we prefer to keep at least one symbolic handle to the main `Module` object.\n",
    "In this way, we will be able to edit the underlying `Module` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34fa5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_migrate_to_gpu(network: nn.Module,\n",
    "                         device:  torch.device,\n",
    "                         n_gpus:  int) -> Tuple[nn.Module, Union[nn.Module, nn.DataParallel]]:\n",
    "    \"\"\"If GPUs are avaiable, migrate the network there for better time performance.\"\"\"\n",
    "\n",
    "    if n_gpus > 0:\n",
    "        network = network.to(device=device)  # move the model parameters to the lead GPU\n",
    "    \n",
    "    if n_gpus > 1:\n",
    "        maybenndp_network = nn.DataParallel(network)  # at runtime, the model will be replicated on each available GPU\n",
    "    else:\n",
    "        maybenndp_network = network\n",
    "    \n",
    "    return network, maybenndp_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6923c",
   "metadata": {},
   "source": [
    "Let's migrate the parameters and hyper-parameters of our MobileNetV2 to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8623678",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnv2, maybenndp_mnv2 = maybe_migrate_to_gpu(mnv2, device, n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32beefcf",
   "metadata": {},
   "source": [
    "### Step 4: evaluate the performance of a raw network\n",
    "\n",
    "As in most experimental sciences, it is crucial to perform elementary experiments also in machine learning.\n",
    "These experiments are important to:\n",
    "* validate assumptions and expectations that, if violated, could invalidate all the following experiments;\n",
    "* establish baselines against which we can compare future results.\n",
    "\n",
    "Given that in the ImageNet validation set the 1000 classes are equally represented, we expect that the accuracy of an untrained MobileNetV2 should be around 0.1%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b4558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label(NamedTuple):\n",
    "    true:      int\n",
    "    predicted: int\n",
    "\n",
    "\n",
    "class Evaluation(OrderedDict):\n",
    "\n",
    "    def __setitem__(self, input_id: int, label: Label):\n",
    "        if not isinstance(input_id, int):\n",
    "            raise TypeError\n",
    "        if not isinstance(label, Label):\n",
    "            raise TypeError\n",
    "\n",
    "        super(Evaluation, self).__setitem__(input_id, label)\n",
    "\n",
    "    @property\n",
    "    def correct(self) -> int:\n",
    "        return sum((label.true == label.predicted) for label in self.values())\n",
    "\n",
    "    @property\n",
    "    def accuracy(self) -> float:\n",
    "        return 100.0 * (float(self.correct) / len(self))\n",
    "\n",
    "    def compare(self, other: Evaluation) -> float:\n",
    "        \"\"\"Return the percentage of matching predictions.\"\"\"\n",
    "\n",
    "        if len(set(self.keys()).symmetric_difference(set(other.keys()))) > 0:\n",
    "            raise ValueError  # can only compare evaluations carried out on the same data points\n",
    "\n",
    "        # else, I proceed with the comparison\n",
    "        matched: int = 0\n",
    "        for input_id, label in self.items():\n",
    "            other_label = other[input_id]\n",
    "            if label.predicted == other_label.predicted:\n",
    "                matched += 1\n",
    "\n",
    "        return 100.0 * (float(matched) / len(self))\n",
    "\n",
    "    \n",
    "def evaluate_network(loader:  torch.utils.data.DataLoader,\n",
    "                     network: Union[nn.Module, nn.DataParallel],\n",
    "                     device:  torch.device) -> Evaluation:\n",
    "\n",
    "    if not isinstance(loader.sampler, torch.utils.data.SequentialSampler):\n",
    "        raise ValueError  # the order of the data points is not deterministic, and the input IDs lose their meaning\n",
    "    \n",
    "    evaluation = Evaluation()\n",
    "    base_input_id: int = 0\n",
    "\n",
    "    for batch_id, (x, y_true) in enumerate(loader):\n",
    "        \n",
    "        x = x.to(device=device)\n",
    "        y_true = y_true.to(device=device)\n",
    "\n",
    "        y_pred = torch.argmax(network(x), dim=1)\n",
    "        \n",
    "        for i, (yt, yp) in enumerate(zip(y_true.flatten(), y_pred.flatten())):\n",
    "            evaluation[base_input_id + i] = Label(int(yt), int(yp))\n",
    "        base_input_id += len(x)\n",
    "        \n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02d4b2",
   "metadata": {},
   "source": [
    "Let's evaluate the network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8c22b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (floating-point, untrained):   0.10%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv2.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv2_perf = evaluate_network(valid_loader, maybenndp_mnv2, device)\n",
    "print(\"Accuracy (floating-point, untrained): {:6.2f}%.\".format(mnv2_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv2.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77bd6f",
   "metadata": {},
   "source": [
    "The accuracy is in line with our expectations: a positive sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3829ac3",
   "metadata": {},
   "source": [
    "## Part 2: training and evaluating a fake-quantised network\n",
    "\n",
    "### Step 1: retrieve a pre-trained floating-point network\n",
    "\n",
    "In some cases, we can apply quantisation algorithms to pre-trained floating-point networks.\n",
    "These algorithms can be classified in:\n",
    "* **post-training quantisation (PTQ)** algorithms, which do not need to run any gradient descent iteration or apply any parameter updates;\n",
    "* **quantisation-aware fine-tuning (QAFT)** algorithms, which are applications of *quantisation-aware training (QAT)* algorithms lasting at most a few epochs.\n",
    "\n",
    "We create a new MobileNetV2 and load such a pre-trained model to speed up our work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18d77675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a MobileNetV2\n",
    "config = 'standard'\n",
    "capacity = 1.0\n",
    "activation = 'relu6'  # other option: `relu`\n",
    "\n",
    "mnv2 = MobileNetV2(config=config, capacity=capacity, activation=activation)\n",
    "mnv2, maybenndp_mnv2 = maybe_migrate_to_gpu(mnv2, device, n_gpus)\n",
    "\n",
    "# get the path to the floating-point checkpoint\n",
    "path_logs = os.path.join(os.curdir, 'logs')\n",
    "fp_checkpoint_filename = '_'.join(['MNv2', str(capacity), activation]) + '.ckpt'\n",
    "path_fp_checkpoint = os.path.join(path_logs, fp_checkpoint_filename)\n",
    "if not os.path.isfile(path_fp_checkpoint):\n",
    "    raise FileNotFoundError\n",
    "\n",
    "# load the pre-trained parameters into the network object\n",
    "pretrained_state_dict = torch.load(path_fp_checkpoint)\n",
    "mnv2.load_state_dict(pretrained_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fee07",
   "metadata": {},
   "source": [
    "Let's evaluate the network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eaeb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (floating-point, trained):  71.86%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv2.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv2_perf = evaluate_network(valid_loader, maybenndp_mnv2, device)\n",
    "print(\"Accuracy (floating-point, trained): {:6.2f}%.\".format(mnv2_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv2.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3027fb",
   "metadata": {},
   "source": [
    "The accuracy is in line with the one reported on the [original paper](https://ieeexplore.ieee.org/document/8578572)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938614e0",
   "metadata": {},
   "source": [
    "### Step 2: perform the float-to-fake (F2F) conversion\n",
    "\n",
    "QuantLib's `editing` package implements the building blocks of a rudimental compiler to transform floating-point `Module`s into quantised neural networks.\n",
    "This package consists of two sub-packages:\n",
    "* `graphs`, extending PyTorch's `nn` and `fx` namespaces;\n",
    "* `editing`, implementing the computational graph annotation and rewriting functionalities.\n",
    "\n",
    "The first step towards quantising our MobileNetV1 is replacing its composing `Module`s with counterparts that support quantisation.\n",
    "In QuantLib, these counterparts are `_QModule`s.\n",
    "\n",
    "As a first step, we need to trace our floating-point network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5abbca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.graphs as qg\n",
    "\n",
    "# trace\n",
    "mnv2.eval()  # remember to freeze parameters, since the `Editor`s might operate with them\n",
    "mnv2fp = qg.fx.quantlib_symbolic_trace(root=mnv2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6fdf4",
   "metadata": {},
   "source": [
    "Now that we have an `fx.GraphModule` object, we can pass it to the tool performing the so-called **float-to-fake (F2F)** conversion flow.\n",
    "\n",
    "In this case, we aim for a quantised network that uses signed 8-bit integers for the weight arrays and unsigned 8-bit integers for the feature arrays.\n",
    "The chosen QAT algorithm is [*parametrised clipping activation (PACT)*](https://proceedings.mlsys.org/paper/2019/file/006f52e9102a8d3be2fe5614f42ba989-Supplemental.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76453995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.editing as qe\n",
    "\n",
    "f2fconverter = qe.f2f.F2F8bitPACTConverter()\n",
    "mnv2fq_uninit = f2fconverter(mnv2fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5b676",
   "metadata": {},
   "source": [
    "Now we are going to prepare all the ingredients to train and validate our fake-quantised network:\n",
    "* the loss function; in PyTorch, loss functions are implemented as `Module`s;\n",
    "* the optimiser; in PyTorch, optimisers are implemented as `torch.optim.Optimizer` objects; the responsibility of `Optimizer`s is updating the parameters, **not** performing gradient descent (which is a prerogative of the [*autograd* engine](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)).\n",
    "\n",
    "Some QAT algorithms, such as PACT, might require specific `Optimizer`s.\n",
    "In these cases, the corresponding QuantLib `algorithms` sub-package must provide their implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b721364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPUs are available, migrate the network\n",
    "mnv2fq_uninit, maybenndp_mnv2fq_uninit = maybe_migrate_to_gpu(mnv2fq_uninit, device, n_gpus)\n",
    "\n",
    "# create the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# create the optimiser; since we use PACT, we need an `Optimizer` capable of updating the clipping bounds independently of other parameters\n",
    "import quantlib.algorithms as qa\n",
    "optimiser = qa.qalgorithms.qatalgorithms.pact.PACTSGD(mnv2fq_uninit, pact_decay=0.001, lr=0.0005, momentum=0.9, weight_decay=4e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44ec45",
   "metadata": {},
   "source": [
    "Multiple iterations through the training data set might be required to bring the model to convergence.\n",
    "Since we also want to validate the performance of our model on the validation set at each training iteration, we define convenient functions to run individual training and validation epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4878c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader:    torch.utils.data.DataLoader,\n",
    "                    network:   Union[nn.Module, nn.DataParallel],\n",
    "                    device:    torch.device,\n",
    "                    loss_fn:   nn.Module,\n",
    "                    optimiser: torch.optim.Optimiser,\n",
    "                    verbose:   bool = False):\n",
    "    \n",
    "    network.train()\n",
    "    \n",
    "    # statistical performance counters  # TODO: define a `StatisticalPerformanceCounters` object\n",
    "    n_points:   int = 0\n",
    "    correct:    int = 0\n",
    "    total_loss: float = 0.0\n",
    "    \n",
    "    for batch_id, (x, y_true) in enumerate(loader):\n",
    "        \n",
    "    ####################\n",
    "        if 100 <= batch_id:\n",
    "            break\n",
    "    ####################\n",
    "        \n",
    "        # cast data points to the network's device\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = network(x)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        \n",
    "        # update performance counters\n",
    "        n_points += len(x)\n",
    "        correct += int(torch.sum(y_true == y_pred.argmax(dim=1)))\n",
    "        total_loss = total_loss + (loss.item() * len(x))\n",
    "        if verbose:\n",
    "            print(\"Training batch [{:5d}/{:5d}] - Loss: {:8.3f} - Accuracy: {:6.2f}%\".format(batch_id, len(loader), total_loss / n_points, 100.0 * (float(correct) / n_points)))\n",
    "        \n",
    "        # backward pass\n",
    "        optimiser.zero_grad()  # clear old gradients\n",
    "        loss.backward()        # compute new gradients\n",
    "        optimiser.step()       # apply gradient descent step\n",
    "        \n",
    "        \n",
    "def validate_one_epoch(loader:    torch.utils.data.DataLoader,\n",
    "                       network:   Union[nn.Module, nn.DataParallel],\n",
    "                       device:    torch.device,\n",
    "                       loss_fn:   nn.Module,\n",
    "                       verbose:   bool = False):\n",
    "    \n",
    "    network.eval()\n",
    "\n",
    "    # statistical performance counters  # TODO: define a `StatisticalPerformanceCounters` object\n",
    "    n_points:   int = 0\n",
    "    correct:    int = 0\n",
    "    total_loss: float = 0.0\n",
    "        \n",
    "    for batch_id, (x, y_true) in enumerate(loader):\n",
    "        \n",
    "        # cast data points to the network's device\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = network(x)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "        # update performance counters\n",
    "        n_points += len(x)\n",
    "        correct += int(torch.sum(y_true == y_pred.argmax(dim=1)))\n",
    "        total_loss = total_loss + (loss.item() * len(x))\n",
    "        if verbose:\n",
    "            print(\"Validation batch [{:5d}/{:5d}] - Loss: {:8.3f} - Accuracy: {:6.2f}%\".format(batch_id, len(loader), total_loss / n_points, 100.0 * (float(correct) / n_points)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003b6c",
   "metadata": {},
   "source": [
    "Before training a fake-quantised network, we want to initialise the hyper-parameters of quantisers to minimise the discrepancy between corresponding floating-point and fake-quantised arrays.\n",
    "\n",
    "To achieve this purpose, we *observe* the statistics of the `Tensor`s passing through the floating-point network during a validation epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0126220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we run this \"observation\" on CPU or a single GPU, we need to limit the batch size\n",
    "warmup_valid_loader = loader_factory.get_dataloader('valid', valid_transform, per_gpu_batch_size, num_workers=n_cpus)\n",
    "\n",
    "# set validation state\n",
    "mnv2fq_uninit.eval()\n",
    "\n",
    "# collect statistics about the floating-point `Tensor`s passing through the quantisers, so that we can better fit the quantisers' hyper-parameters\n",
    "# start observing\n",
    "for m in mnv2fq_uninit.modules():\n",
    "    if isinstance(m, tuple(qa.qalgorithms.qatalgorithms.pact.NNMODULE_TO_PACTMODULE.values())):\n",
    "        m.start_observing()\n",
    "# collect statistics\n",
    "validate_one_epoch(warmup_valid_loader, mnv2fq_uninit, device, loss_fn)\n",
    "# stop observing\n",
    "for m in mnv2fq_uninit.modules():\n",
    "    if isinstance(m, tuple(qa.qalgorithms.qatalgorithms.pact.NNMODULE_TO_PACTMODULE.values())):\n",
    "        m.stop_observing()\n",
    "\n",
    "# restore training state\n",
    "mnv2fq_uninit.train()\n",
    "\n",
    "mnv2fq_init, maybenndp_mnv2fq_init = mnv2fq_uninit, maybenndp_mnv2fq_uninit  # now the quantisers' hyper-parameters are initialised\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1bda2",
   "metadata": {},
   "source": [
    "Note that this tuning operates locally (i.e., on pairs of corresponding arrays).\n",
    "Therefore, the discrepancies will likely propagate through the network yielding poor performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4b6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (fake-quantised, untrained):   0.24%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv2fq_init.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv2fq_perf = evaluate_network(valid_loader, maybenndp_mnv2fq_init, device)\n",
    "print(\"Accuracy (fake-quantised, untrained): {:6.2f}%.\".format(mnv2fq_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv2fq_init.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9707b06",
   "metadata": {},
   "source": [
    "Let's proceed with a fine-tuning epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "042ae5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch [    0/13346] - Loss:    1.500 - Accuracy:  63.54%\n",
      "Training batch [    1/13346] - Loss:    1.551 - Accuracy:  59.90%\n",
      "Training batch [    2/13346] - Loss:    1.503 - Accuracy:  63.89%\n",
      "Training batch [    3/13346] - Loss:    1.608 - Accuracy:  61.98%\n",
      "Training batch [    4/13346] - Loss:    1.649 - Accuracy:  61.88%\n",
      "Training batch [    5/13346] - Loss:    1.588 - Accuracy:  63.37%\n",
      "Training batch [    6/13346] - Loss:    1.566 - Accuracy:  63.69%\n",
      "Training batch [    7/13346] - Loss:    1.568 - Accuracy:  63.54%\n",
      "Training batch [    8/13346] - Loss:    1.610 - Accuracy:  62.85%\n",
      "Training batch [    9/13346] - Loss:    1.583 - Accuracy:  63.54%\n",
      "Training batch [   10/13346] - Loss:    1.569 - Accuracy:  63.92%\n",
      "Training batch [   11/13346] - Loss:    1.559 - Accuracy:  64.24%\n",
      "Training batch [   12/13346] - Loss:    1.521 - Accuracy:  64.82%\n",
      "Training batch [   13/13346] - Loss:    1.527 - Accuracy:  64.73%\n",
      "Training batch [   14/13346] - Loss:    1.523 - Accuracy:  64.79%\n",
      "Training batch [   15/13346] - Loss:    1.526 - Accuracy:  64.71%\n",
      "Training batch [   16/13346] - Loss:    1.514 - Accuracy:  64.58%\n",
      "Training batch [   17/13346] - Loss:    1.522 - Accuracy:  64.06%\n",
      "Training batch [   18/13346] - Loss:    1.508 - Accuracy:  64.36%\n",
      "Training batch [   19/13346] - Loss:    1.514 - Accuracy:  64.53%\n",
      "Training batch [   20/13346] - Loss:    1.512 - Accuracy:  64.68%\n",
      "Training batch [   21/13346] - Loss:    1.499 - Accuracy:  64.82%\n",
      "Training batch [   22/13346] - Loss:    1.501 - Accuracy:  64.90%\n",
      "Training batch [   23/13346] - Loss:    1.490 - Accuracy:  65.19%\n",
      "Training batch [   24/13346] - Loss:    1.500 - Accuracy:  65.17%\n",
      "Training batch [   25/13346] - Loss:    1.502 - Accuracy:  65.30%\n",
      "Training batch [   26/13346] - Loss:    1.501 - Accuracy:  65.32%\n",
      "Training batch [   27/13346] - Loss:    1.484 - Accuracy:  65.66%\n",
      "Training batch [   28/13346] - Loss:    1.499 - Accuracy:  65.34%\n",
      "Training batch [   29/13346] - Loss:    1.494 - Accuracy:  65.31%\n",
      "Training batch [   30/13346] - Loss:    1.490 - Accuracy:  65.46%\n",
      "Training batch [   31/13346] - Loss:    1.504 - Accuracy:  65.27%\n",
      "Training batch [   32/13346] - Loss:    1.506 - Accuracy:  65.25%\n",
      "Training batch [   33/13346] - Loss:    1.499 - Accuracy:  65.29%\n",
      "Training batch [   34/13346] - Loss:    1.501 - Accuracy:  65.15%\n",
      "Training batch [   35/13346] - Loss:    1.505 - Accuracy:  65.10%\n",
      "Training batch [   36/13346] - Loss:    1.508 - Accuracy:  64.92%\n",
      "Training batch [   37/13346] - Loss:    1.507 - Accuracy:  64.91%\n",
      "Training batch [   38/13346] - Loss:    1.508 - Accuracy:  64.90%\n",
      "Training batch [   39/13346] - Loss:    1.510 - Accuracy:  64.79%\n",
      "Training batch [   40/13346] - Loss:    1.509 - Accuracy:  64.84%\n",
      "Training batch [   41/13346] - Loss:    1.517 - Accuracy:  64.63%\n",
      "Training batch [   42/13346] - Loss:    1.516 - Accuracy:  64.66%\n",
      "Training batch [   43/13346] - Loss:    1.512 - Accuracy:  64.65%\n",
      "Training batch [   44/13346] - Loss:    1.508 - Accuracy:  64.70%\n",
      "Training batch [   45/13346] - Loss:    1.506 - Accuracy:  64.79%\n",
      "Training batch [   46/13346] - Loss:    1.500 - Accuracy:  64.87%\n",
      "Training batch [   47/13346] - Loss:    1.504 - Accuracy:  64.80%\n",
      "Training batch [   48/13346] - Loss:    1.501 - Accuracy:  64.90%\n",
      "Training batch [   49/13346] - Loss:    1.498 - Accuracy:  65.02%\n",
      "Training batch [   50/13346] - Loss:    1.500 - Accuracy:  65.05%\n",
      "Training batch [   51/13346] - Loss:    1.493 - Accuracy:  65.24%\n",
      "Training batch [   52/13346] - Loss:    1.495 - Accuracy:  65.27%\n",
      "Training batch [   53/13346] - Loss:    1.495 - Accuracy:  65.20%\n",
      "Training batch [   54/13346] - Loss:    1.493 - Accuracy:  65.27%\n",
      "Training batch [   55/13346] - Loss:    1.491 - Accuracy:  65.20%\n",
      "Training batch [   56/13346] - Loss:    1.493 - Accuracy:  65.26%\n",
      "Training batch [   57/13346] - Loss:    1.494 - Accuracy:  65.30%\n",
      "Training batch [   58/13346] - Loss:    1.499 - Accuracy:  65.17%\n",
      "Training batch [   59/13346] - Loss:    1.495 - Accuracy:  65.24%\n",
      "Training batch [   60/13346] - Loss:    1.499 - Accuracy:  65.25%\n",
      "Training batch [   61/13346] - Loss:    1.496 - Accuracy:  65.32%\n",
      "Training batch [   62/13346] - Loss:    1.494 - Accuracy:  65.39%\n",
      "Training batch [   63/13346] - Loss:    1.494 - Accuracy:  65.36%\n",
      "Training batch [   64/13346] - Loss:    1.499 - Accuracy:  65.22%\n",
      "Training batch [   65/13346] - Loss:    1.498 - Accuracy:  65.25%\n",
      "Training batch [   66/13346] - Loss:    1.499 - Accuracy:  65.27%\n",
      "Training batch [   67/13346] - Loss:    1.498 - Accuracy:  65.29%\n",
      "Training batch [   68/13346] - Loss:    1.498 - Accuracy:  65.29%\n",
      "Training batch [   69/13346] - Loss:    1.498 - Accuracy:  65.34%\n",
      "Training batch [   70/13346] - Loss:    1.497 - Accuracy:  65.40%\n",
      "Training batch [   71/13346] - Loss:    1.495 - Accuracy:  65.38%\n",
      "Training batch [   72/13346] - Loss:    1.496 - Accuracy:  65.31%\n",
      "Training batch [   73/13346] - Loss:    1.496 - Accuracy:  65.22%\n",
      "Training batch [   74/13346] - Loss:    1.497 - Accuracy:  65.18%\n",
      "Training batch [   75/13346] - Loss:    1.505 - Accuracy:  65.08%\n",
      "Training batch [   76/13346] - Loss:    1.503 - Accuracy:  65.12%\n",
      "Training batch [   77/13346] - Loss:    1.500 - Accuracy:  65.09%\n",
      "Training batch [   78/13346] - Loss:    1.501 - Accuracy:  65.01%\n",
      "Training batch [   79/13346] - Loss:    1.501 - Accuracy:  64.96%\n",
      "Training batch [   80/13346] - Loss:    1.504 - Accuracy:  64.90%\n",
      "Training batch [   81/13346] - Loss:    1.501 - Accuracy:  64.96%\n",
      "Training batch [   82/13346] - Loss:    1.502 - Accuracy:  64.93%\n",
      "Training batch [   83/13346] - Loss:    1.499 - Accuracy:  65.03%\n",
      "Training batch [   84/13346] - Loss:    1.501 - Accuracy:  64.96%\n",
      "Training batch [   85/13346] - Loss:    1.507 - Accuracy:  64.86%\n",
      "Training batch [   86/13346] - Loss:    1.507 - Accuracy:  64.93%\n",
      "Training batch [   87/13346] - Loss:    1.504 - Accuracy:  65.02%\n",
      "Training batch [   88/13346] - Loss:    1.501 - Accuracy:  65.06%\n",
      "Training batch [   89/13346] - Loss:    1.501 - Accuracy:  65.03%\n",
      "Training batch [   90/13346] - Loss:    1.502 - Accuracy:  65.10%\n",
      "Training batch [   91/13346] - Loss:    1.505 - Accuracy:  65.04%\n",
      "Training batch [   92/13346] - Loss:    1.502 - Accuracy:  65.13%\n",
      "Training batch [   93/13346] - Loss:    1.502 - Accuracy:  65.14%\n",
      "Training batch [   94/13346] - Loss:    1.501 - Accuracy:  65.16%\n",
      "Training batch [   95/13346] - Loss:    1.502 - Accuracy:  65.19%\n",
      "Training batch [   96/13346] - Loss:    1.498 - Accuracy:  65.26%\n",
      "Training batch [   97/13346] - Loss:    1.499 - Accuracy:  65.23%\n",
      "Training batch [   98/13346] - Loss:    1.499 - Accuracy:  65.23%\n",
      "Training batch [   99/13346] - Loss:    1.496 - Accuracy:  65.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_checkpoint_base_filename, extension = fp_checkpoint_filename.rsplit('.', 1)\n",
    "fq_checkpoint_base_filename = '_'.join([fp_checkpoint_base_filename, 'FQ', 'uint8x', 'int8w'])\n",
    "fq_checkpoint_filename = '.'.join([fq_checkpoint_base_filename, extension])\n",
    "\n",
    "path_fq_checkpoint = os.path.join(path_logs, fq_checkpoint_filename)\n",
    "\n",
    "if True:  # not os.path.isfile(path_fq_checkpoint):\n",
    "    # TODO: There is an incompatibility issue with the keys including\n",
    "    #       QuantLib-generated IDs: since these IDs are built from the\n",
    "    #       IDs of Python objects, different runs will assign different\n",
    "    #       IDs to composing `Module`s. We might want to consider\n",
    "    #       whether fake-quantised models should be loadable; in this\n",
    "    #       case, we will need to engineer a generation of `fx.Node`\n",
    "    #       \"targets\" that does depend on the network, but not on the\n",
    "    #       `Editor` that performed the modification.\n",
    "    train_one_epoch(train_loader, maybenndp_mnv2fq_init, device, loss_fn, optimiser, verbose=True)\n",
    "    torch.save(mnv2fq_init.state_dict(), path_fq_checkpoint)\n",
    "\n",
    "mnv2fq_init.load_state_dict(torch.load(path_fq_checkpoint))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dad794",
   "metadata": {},
   "source": [
    "Before we measure the performance of our MobileNetV2, we need to make sure that the scales of the fake-quantised `Tensor`s being added together at the end of residual blocks are equal (i.e., *harmonised*).\n",
    "This property ensures that additions between fake-quantised `Tensor`s can be mapped to additions between integer arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60121430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QL_AddTreeHarmoniser[140503707525280][1]\n",
      "tensor([0.0854], device='cuda:0')\n",
      "tensor([0.0854], device='cuda:0')\n",
      "tensor([0.0854], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][2]\n",
      "tensor([0.0653], device='cuda:0')\n",
      "tensor([0.0653], device='cuda:0')\n",
      "tensor([0.0653], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][3]\n",
      "tensor([0.0777], device='cuda:0')\n",
      "tensor([0.0777], device='cuda:0')\n",
      "tensor([0.0777], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][4]\n",
      "tensor([0.0389], device='cuda:0')\n",
      "tensor([0.0389], device='cuda:0')\n",
      "tensor([0.0389], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][5]\n",
      "tensor([0.0424], device='cuda:0')\n",
      "tensor([0.0424], device='cuda:0')\n",
      "tensor([0.0424], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][6]\n",
      "tensor([0.0431], device='cuda:0')\n",
      "tensor([0.0431], device='cuda:0')\n",
      "tensor([0.0431], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][7]\n",
      "tensor([0.0416], device='cuda:0')\n",
      "tensor([0.0416], device='cuda:0')\n",
      "tensor([0.0416], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][8]\n",
      "tensor([0.0758], device='cuda:0')\n",
      "tensor([0.0758], device='cuda:0')\n",
      "tensor([0.0758], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][9]\n",
      "tensor([0.1398], device='cuda:0')\n",
      "tensor([0.1398], device='cuda:0')\n",
      "tensor([0.1398], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][10]\n",
      "tensor([0.2895], device='cuda:0')\n",
      "tensor([0.2895], device='cuda:0')\n",
      "tensor([0.2895], device='cuda:0')\n",
      "Accuracy (fake-quantised, trained):  67.59%.\n",
      "QL_AddTreeHarmoniser[140503707525280][1]\n",
      "tensor([0.0854], device='cuda:0')\n",
      "tensor([0.0854], device='cuda:0')\n",
      "tensor([0.0854], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][2]\n",
      "tensor([0.0653], device='cuda:0')\n",
      "tensor([0.0653], device='cuda:0')\n",
      "tensor([0.0653], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][3]\n",
      "tensor([0.0777], device='cuda:0')\n",
      "tensor([0.0777], device='cuda:0')\n",
      "tensor([0.0777], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][4]\n",
      "tensor([0.0389], device='cuda:0')\n",
      "tensor([0.0389], device='cuda:0')\n",
      "tensor([0.0389], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][5]\n",
      "tensor([0.0424], device='cuda:0')\n",
      "tensor([0.0424], device='cuda:0')\n",
      "tensor([0.0424], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][6]\n",
      "tensor([0.0431], device='cuda:0')\n",
      "tensor([0.0431], device='cuda:0')\n",
      "tensor([0.0431], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][7]\n",
      "tensor([0.0416], device='cuda:0')\n",
      "tensor([0.0416], device='cuda:0')\n",
      "tensor([0.0416], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][8]\n",
      "tensor([0.0758], device='cuda:0')\n",
      "tensor([0.0758], device='cuda:0')\n",
      "tensor([0.0758], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][9]\n",
      "tensor([0.1398], device='cuda:0')\n",
      "tensor([0.1398], device='cuda:0')\n",
      "tensor([0.1398], device='cuda:0')\n",
      "QL_AddTreeHarmoniser[140503707525280][10]\n",
      "tensor([0.2895], device='cuda:0')\n",
      "tensor([0.2895], device='cuda:0')\n",
      "tensor([0.2895], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv2fq_init.eval()\n",
    "##################\n",
    "for n, m in mnv2fq_init.named_modules():\n",
    "    if isinstance(m, qg.nn.HarmonisedAdd):\n",
    "        m.harmonise()\n",
    "        print(n)\n",
    "        print(m._input_qmodules[0].scale)\n",
    "        print(m._input_qmodules[1].scale)\n",
    "        print(m._output_qmodule.scale)\n",
    "##################\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv2fq_init_perf = evaluate_network(valid_loader, maybenndp_mnv2fq_init, device)\n",
    "print(\"Accuracy (fake-quantised, trained): {:6.2f}%.\".format(mnv2fq_init_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv2fq_init.train()\n",
    "pass\n",
    "\n",
    "##################\n",
    "for n, m in mnv2fq_init.named_modules():\n",
    "    if isinstance(m, qg.nn.HarmonisedAdd):\n",
    "        print(n)\n",
    "        print(m._input_qmodules[0].scale)\n",
    "        print(m._input_qmodules[1].scale)\n",
    "        print(m._output_qmodule.scale)\n",
    "##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f7c1b",
   "metadata": {},
   "source": [
    "Our fake-quantised MobileNetV2 is less accurate than the original floating-point one.\n",
    "Nonetheless, we might agree that it is far from random (and therefore sufficient for demonstration purposes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb286627",
   "metadata": {},
   "source": [
    "## Part 3: integerising a fake-quantised network\n",
    "\n",
    "A trained fake-quantised network is not a true integerised program.\n",
    "To obtain such a network, we must apply the so-called **fake-to-true (F2T)** conversion.\n",
    "An F2T conversion is a sequence of program transformations to turn a fake-quantised network into an integerised one.\n",
    "\n",
    "At the current stage, QuantLib does not support fully-automatic integerisation of programs due to network-specific computation patterns that we have not yet broken down into atomic transforms.\n",
    "Usually, we can find these network-specific patterns in the most downstream parts of a network; indeed, network *backbones* are general-purpose, whereas the output *heads* are task-specific and can create exotic computation patterns.\n",
    "\n",
    "Before we proceed with F2T conversion, we need to specify how to rewrite the classifier of our MobileNetV2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6ba3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNv2Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MNv2Head, self).__init__()\n",
    "        self.eps = qg.nn.EpsTunnel(torch.Tensor([1.0]))\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.drp = nn.Dropout(0.2)\n",
    "        self.lin = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.eps(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drp(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MNv2HeadApplier(qe.editors.nnmodules.NNModuleApplier):\n",
    "\n",
    "    def __init__(self, mnv2headpattern: qe.editors.nnmodules.GenericNNModulePattern):\n",
    "        super(MNv2HeadApplier, self).__init__(mnv2headpattern)\n",
    "\n",
    "    def _apply(self, g: fx.GraphModule, ap: qe.editors.nnmodules.NodesMap, id_: str) -> fx.GraphModule:\n",
    "\n",
    "        name_to_match_node = self.pattern.name_to_match_node(nodes_map=ap)\n",
    "        node_drp = name_to_match_node['drp']\n",
    "        node_lin = name_to_match_node['lin']\n",
    "\n",
    "        name_to_match_module = self.pattern.name_to_match_module(nodes_map=ap, data_gm=g)\n",
    "        module_eps = name_to_match_module['eps']\n",
    "        module_lin = name_to_match_module['lin']\n",
    "\n",
    "        assert module_eps.eps_out.numel() == 1\n",
    "        assert len(node_drp.all_input_nodes) == 1\n",
    "        assert len(node_lin.all_input_nodes) == 1\n",
    "\n",
    "        # create the new module\n",
    "        new_target = id_\n",
    "        new_module = nn.Linear(in_features=module_lin.in_features, out_features=module_lin.out_features, bias=module_lin.bias is not None)\n",
    "        new_weight = module_lin.weight.data.detach().clone() * module_eps.eps_out\n",
    "        new_module.weight.data = new_weight\n",
    "        if module_lin.bias is not None:\n",
    "            new_bias = module_lin.bias.data.detach().clone()\n",
    "            new_module.bias.data = new_bias\n",
    "\n",
    "        # add the requantised linear operation to the graph...\n",
    "        g.add_submodule(new_target, new_module)\n",
    "        drp_input = next(iter(node_drp.all_input_nodes))\n",
    "        with g.graph.inserting_after(drp_input):\n",
    "            new_node = g.graph.call_module(new_target, args=(drp_input,))\n",
    "        node_lin.replace_all_uses_with(new_node)\n",
    "\n",
    "        module_eps.set_eps_out(torch.ones_like(module_eps.eps_out))\n",
    "\n",
    "        # ...and delete the old operations\n",
    "        g.delete_submodule(node_lin.target)\n",
    "        g.graph.erase_node(node_lin)\n",
    "        g.delete_submodule(node_drp.target)\n",
    "        g.graph.erase_node(node_drp)\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "class MNv2HeadRewriter(qe.editors.nnmodules.NNModuleRewriter):\n",
    "\n",
    "    def __init__(self):\n",
    "        # create pattern\n",
    "        mnv2headwithcheckers = qe.editors.nnmodules.NNModuleWithCheckers(MNv2Head(), {})\n",
    "        mnv2headpattern = qe.editors.nnmodules.GenericNNModulePattern(qg.fx.quantlib_symbolic_trace, mnv2headwithcheckers)\n",
    "        # create matcher and applier\n",
    "        finder = qe.editors.nnmodules.GenericGraphMatcher(mnv2headpattern)\n",
    "        applier = MNv2HeadApplier(mnv2headpattern)\n",
    "        # link pattern, matcher, and applier into the rewriter\n",
    "        super(MNv2HeadRewriter, self).__init__('MNv2HeadRewriter', mnv2headpattern, finder, applier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88fe80",
   "metadata": {},
   "source": [
    "We are ready to apply F2T conversion.\n",
    "\n",
    "When creating an `F2TConverter`, users can pass a `custom_editor` to perform model-specific rewritings.\n",
    "This transform will be applied right before removing identity operations from the target `GraphModule`.\n",
    "\n",
    "As part of its logic, F2T conversion must perform some semantic analysis of the input `GraphModule`.\n",
    "Since `F2TConverter`s can not infer all the semantics automatically, the user must feed the shape and floating-point scale of the network's inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa52eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/quantlib-0.2-py3.8.egg/quantlib/editing/editing/editors/nnmodules/finder/nnsequential.py:215: UserWarning: [QuantLib warning] RequantiserMatcher: two matches with non-zero overlap were found; the most recently discovered will be discarded.\n",
      "  warnings.warn(quantlib_wng_header(obj_name=self.__class__.__name__) + \"two matches with non-zero overlap were found; the most recently discovered will be discarded.\")\n"
     ]
    }
   ],
   "source": [
    "# F2T conversion and ONNX exporting require structural information about the input\n",
    "x, _ = next(iter(valid_loader))\n",
    "x = x[0].unsqueeze(0)\n",
    "\n",
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "mnv2fq_init = mnv2fq_init.to(torch.device('cpu'))  # TODO: the `Tensor`s generated inside the F2T conversion flow are generated for CPU\n",
    "mnv2fq_init.eval()\n",
    "\n",
    "# perform the conversion\n",
    "f2tconverter = qe.f2t.F2T24bitConverter(custom_editor=MNv2HeadRewriter())\n",
    "mnv2tq = f2tconverter(mnv2fq_init, {'x': {'shape': x.shape, 'scale': torch.Tensor([ImageNetStats['quantise']['scale']])}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f813076",
   "metadata": {},
   "source": [
    "To validate the performance of the integerised network, we need to pass it integerised data.\n",
    "Thus, we create a `DataLoader` yielding `Tensor` images with integer components.\n",
    "\n",
    "Although these integers are represented as floating-point numbers, we note that the ranges of typical digital data points (e.g., INT8 or UINT8) can be represented without any loss when embedded in standard floating-point ranges (e.g., FP32 or FP64).\n",
    "Thus, although networks integerised using QuantLib still use floating-point arithmetic, in practice, we observed that their outputs coincide with those of truly integer networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bcfe95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the validation `DataLoader` returning integerised (UINT8) images\n",
    "int_valid_transform = ImageNetTransform('valid', integerise=True)\n",
    "int_valid_loader = loader_factory.get_dataloader('valid', int_valid_transform, per_gpu_batch_size, num_workers=n_cpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41824c69",
   "metadata": {},
   "source": [
    "Let's evaluate our true-quantised MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d20c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (true-quantised, trained):  65.41%.\n"
     ]
    }
   ],
   "source": [
    "mnv2tq = mnv2tq.to(device=device)\n",
    "\n",
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "mnv2tq.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv2tq_perf = evaluate_network(int_valid_loader, mnv2tq, device)\n",
    "print(\"Accuracy (true-quantised, trained): {:6.2f}%.\".format(mnv2tq_perf.accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf73785",
   "metadata": {},
   "source": [
    "The performance is equivalent to (and, in some runs, even better than) that of the corresponding fake-quantised network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e8553",
   "metadata": {},
   "source": [
    "## Part 4: export a backend-specific ONNX model\n",
    "\n",
    "The QuantLib `backends` package contains the abstractions required to export **ONNX** models and annotate them with backend-specific information.\n",
    "To demonstrate its usage, we consider the [DORY](https://github.com/pulp-platform/dory) backend.\n",
    "\n",
    "First, we create the folder to host DORY-specific files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7827655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "backend_name = 'DORY'\n",
    "path_export = os.path.join(os.curdir, backend_name)\n",
    "\n",
    "if os.path.isdir(path_export):  # remove old files\n",
    "    shutil.rmtree(path_export)\n",
    "os.mkdir(path_export)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c1963",
   "metadata": {},
   "source": [
    "Then, we export an annotated ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2ec9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.backends as qb\n",
    "\n",
    "x = x.to(torch.device('cpu'))\n",
    "mnv2tq = mnv2tq.to(device=torch.device('cpu'))\n",
    "\n",
    "exporter = qb.dory.DORYExporter()\n",
    "exporter.export(network=mnv2tq, input_shape=x.shape, path=path_export)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95592e71",
   "metadata": {},
   "source": [
    "We can use [Netron](https://netron.app/) to visually inspect that the exported model is an integerised program.\n",
    "\n",
    "DORY can also verify whether the exported ONNX model can be compiled into an integerised program for PULP platforms.\n",
    "To perform this consistency check, users must dump the input and features maps associated with an example data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "944a96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter.dump_features(network=mnv2tq, x=x, path=path_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a363f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have reached the bottom of the part of the deep learning stack covered by QuantLib.\n",
    "If you are interested in graph optimisations and code generation, you can read the [DORY paper](https://ieeexplore.ieee.org/document/9381618) and check out the DORY repository.\n",
    "\n",
    "Cheers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddaea19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
