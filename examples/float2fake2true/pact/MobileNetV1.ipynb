{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c91f862",
   "metadata": {},
   "source": [
    "# Quantising a MobileNetV1 with the PACT algorithm\n",
    "\n",
    "In this notebook, we show how to create an integerised MobileNetV1 using the QuantLib package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b866f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import NamedTuple, List, Union, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8065fc",
   "metadata": {},
   "source": [
    "## Part 1: creating and evaluating a floating-point network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588f570",
   "metadata": {},
   "source": [
    "### Step 1: check the computing infrastructure\n",
    "\n",
    "Depending on the hardware we have, we will make different choices about the training and testing processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf78ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available system CPU(s): 20.\n",
      "Available system GPU(s): 2.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "print(f\"Available system CPU(s): {n_cpus}.\")\n",
    "print(f\"Available system GPU(s): {n_gpus}.\")\n",
    "\n",
    "device = torch.device(torch.cuda.current_device()) if (n_gpus > 0) else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314107e5",
   "metadata": {},
   "source": [
    "### Step 2: create PyTorch data loaders\n",
    "\n",
    "Each supervised learning problem partitions data points into three categories:\n",
    "* **training** points: they are available at training time, and are consumed by the learning algorithm to update the model's parameters;\n",
    "* **validation** points: they are available at training time, they are not used to update the parameters, but they can be used to measure the performance of the model throughout the learning process;\n",
    "* **test** points: they are not available at training time, but are available only once the model is deployed in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ebacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto, unique\n",
    "\n",
    "\n",
    "# each supervised learning problem defines a training/validation/test partition of its data points\n",
    "@unique\n",
    "class DataSetPartition(Enum):\n",
    "    TRAINING   = auto()\n",
    "    VALIDATION = auto()\n",
    "    TEST       = auto()\n",
    "\n",
    "    @classmethod\n",
    "    def canonicalise(cls, partition_spec: DataSetPartitionSpec) -> DataSetPartition:\n",
    "\n",
    "        # validate specification type\n",
    "        if not isinstance(partition_spec, (DataSetPartition, str,)):\n",
    "            raise TypeError\n",
    "\n",
    "        if isinstance(partition_spec, DataSetPartition):\n",
    "            partition = partition_spec\n",
    "        \n",
    "        else:  # `isinstance(partition, str)`\n",
    "\n",
    "            partition_spec = partition_spec.upper()\n",
    "            if partition_spec in {'TRAINING', 'TRAIN'}:\n",
    "                partition = cls['TRAINING']\n",
    "            elif partition_spec in {'VALIDATION', 'VALID'}:\n",
    "                partition = cls['VALIDATION']\n",
    "            elif partition_spec in {'TEST', 'TESTING'}:\n",
    "                partition = cls['TEST']\n",
    "            else:\n",
    "                raise ValueError\n",
    "        \n",
    "        return partition\n",
    "\n",
    "    \n",
    "# define the ways in which a user can specify data set partitions\n",
    "DataSetPartitionSpec = Union[DataSetPartition, str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dce90c",
   "metadata": {},
   "source": [
    "PyTorch represents data points using the `torch.Tensor` data structure.\n",
    "PyTorch uses a specific pipeline to transform files stored on disk into mini-batches of `torch.Tensor`s.\n",
    "This pipeline consists of three stages.\n",
    "* Define a function to transform each data point stored on disk into a corresponding collection of `torch.Tensor`s; for instance, a labelled image file can be mapped to a pair consisting of an input tensor and a label tensor. These functions are called *transforms*, and might be composed to describe complex pre-processing transformations (see `torchvision.transforms.Compose`). Since *transforms* can be applied individually to each data point, `torch.utils.data.Dataset` objects apply a *map pattern* to load the files on disk into a Python list.\n",
    "* Define a `torch.utils.data.Sampler` that can sample and return mini-batches from a given list of integers. This batching can be performed with or without permuting the list; if the list is permuted, it can be permuted once (without repetition), or in-between each sampling (with repetition).\n",
    "* Define a `torch.utils.data.DataLoader` specifying the size of the mini-batches and how many worker threads should be used to load data point files from disk. The `DataLoader` queries the `Sampler` for a list of indices, distributes these indices to the worker threads, each thread returns the `Tensor` data points, the `DataLoader` collates them into a mini-batch of `Tensor`s, and returns these mini-batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687e77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class ImageNetDataLoaderFactory(object):\n",
    "    \n",
    "    def __init__(self, path_data: str):\n",
    "        \n",
    "        if not os.path.isdir(path_data):\n",
    "            raise FileNotFounderror  # missing ImageNet data folder\n",
    "\n",
    "        super(ImageNetDataLoaderFactory, self).__init__()\n",
    "\n",
    "        self._partition_to_subfolder = OrderedDict([\n",
    "            (DataSetPartition['TRAINING'], os.path.join(path_data, 'train')),\n",
    "            (DataSetPartition['VALIDATION'], os.path.join(path_data, 'val')),\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def get_dataset(self,\n",
    "                    partition_spec: DataSetPartitionSpec,\n",
    "                    transform:      torchvision.transforms.Compose) -> torch.utils.data.Dataset:\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "        return torchvision.datasets.ImageFolder(self._partition_to_subfolder[partition], transform)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_sampler(partition_spec: DataSetPartitionSpec,\n",
    "                    dataset: torch.utils.data.Dataset) -> torch.utils.data.Sampler:\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "        return torch.utils.data.RandomSampler(dataset) if (partition is DataSetPartition['TRAINING']) else torch.utils.data.SequentialSampler(dataset)\n",
    "    \n",
    "    def get_dataloader(self,\n",
    "                       partition_spec: DataSetPartitionSpec,\n",
    "                       transform:      torchvision.transforms.Compose,\n",
    "                       batch_size:     int,\n",
    "                       num_workers:    int = 1) -> torch.utils.data.DataLoader:\n",
    "\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "\n",
    "        dataset = self.get_dataset(partition, transform)\n",
    "        sampler = ImageNetDataLoaderFactory.get_sampler(partition, dataset)\n",
    "        loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                             sampler=sampler,\n",
    "                                             batch_size=batch_size,\n",
    "                                             num_workers=num_workers)\n",
    "\n",
    "        return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a99ee1",
   "metadata": {},
   "source": [
    "Now, we define ImageNet-specific transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5b8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomHorizontalFlip, RandomResizedCrop\n",
    "from torchvision.transforms import Resize, CenterCrop\n",
    "from torchvision.transforms import ToTensor, Normalize, Lambda\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "ImageNetStats = \\\n",
    "    {\n",
    "        'normalise':\n",
    "            {\n",
    "                'mean': (0.485, 0.456, 0.406),\n",
    "                'std':  (0.229, 0.224, 0.225)\n",
    "            },\n",
    "        'quantise':\n",
    "            {\n",
    "                'min':   -2.1179039478,  # computed on the normalised images of the validation partition\n",
    "                'max':   2.6400001049,   # computed on the normalised images of the validation partition\n",
    "                'scale': 0.020625000819563866\n",
    "            }\n",
    "    }\n",
    "\n",
    "\n",
    "class ImageNetNormalise(Normalize):\n",
    "    def __init__(self):\n",
    "        super(ImageNetNormalise, self).__init__(**ImageNetStats['normalise'])\n",
    "        \n",
    "\n",
    "class ImageNetIntegerise(Lambda):\n",
    "    def __init__(self):\n",
    "        INT8_MIN = -2**(8-1)\n",
    "        INT8_MAX = 2**(8-1) - 1\n",
    "        image_scale = ImageNetStats['quantise']['scale']\n",
    "        super(ImageNetIntegerise, self).__init__(lambda x: torch.clip((x / image_scale).floor(), INT8_MIN, INT8_MAX))\n",
    "\n",
    "\n",
    "class ImageNetTransform(Compose):\n",
    "\n",
    "    def __init__(self, partition_spec: DataSetPartitionSpec, image_size: int = 224, integerise: bool = False):\n",
    "\n",
    "        # validate arguments\n",
    "        RESIZE_SIZE = 256\n",
    "        if not (image_size <= RESIZE_SIZE):\n",
    "            raise ValueError  # otherwise, we can not crop the resized image to the desired size\n",
    "\n",
    "        partition = DataSetPartition.canonicalise(partition_spec)\n",
    "\n",
    "        if partition is DataSetPartition['TRAINING']:\n",
    "            transforms = [RandomHorizontalFlip(),\n",
    "                          RandomResizedCrop(image_size)]\n",
    "        else:\n",
    "            transforms = [Resize(RESIZE_SIZE),\n",
    "                          CenterCrop(image_size)]\n",
    "\n",
    "        transforms += [ToTensor(),\n",
    "                       ImageNetNormalise(),\n",
    "                       ImageNetIntegerise()]\n",
    "        \n",
    "        if not integerise:\n",
    "            transforms += [Lambda(lambda x: x * ImageNetStats['quantise']['scale'])]  # return a fake-quantised `Tensor`\n",
    "\n",
    "        super(ImageNetTransform, self).__init__(transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac188a",
   "metadata": {},
   "source": [
    "We are now ready to create our training and validation `DataLoader`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc13a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the `DataLoader` factory\n",
    "path_data = os.path.join(os.curdir, 'data')\n",
    "loader_factory = ImageNetDataLoaderFactory(path_data)\n",
    "\n",
    "batch_size = 128 * n_gpus\n",
    "\n",
    "# create the training `DataLoader`\n",
    "train_transform = ImageNetTransform('train')\n",
    "train_loader = loader_factory.get_dataloader('train', train_transform, batch_size, num_workers=n_cpus)\n",
    "\n",
    "# create the validation `DataLoader`\n",
    "valid_transform = ImageNetTransform('valid')\n",
    "valid_loader = loader_factory.get_dataloader('valid', valid_transform, batch_size, num_workers=n_cpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412f36c",
   "metadata": {},
   "source": [
    "### Step 3: create a floating-point PyTorch network\n",
    "\n",
    "PyTorch represents deep neural networks as `torch.nn.Module`s.\n",
    "Since deep neural networks can be (and are often) modelled as function compositions, `nn.Module`s can be composed to create complex functions, i.e., complex networks.\n",
    "\n",
    "MobileNetV1 is a sequential composition of `nn.Module`s, therefore it is a relatively simple network to quantise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630b4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "CONFIGS = OrderedDict([\n",
    "    ('STANDARD', [\n",
    "        ( 2, 1),\n",
    "        ( 4, 2),\n",
    "        ( 4, 1),\n",
    "        ( 8, 2),\n",
    "        ( 8, 1),\n",
    "        (16, 2),\n",
    "        (16, 1),\n",
    "        (16, 1),\n",
    "        (16, 1),\n",
    "        (16, 1),\n",
    "        (16, 1),\n",
    "        (32, 2),\n",
    "        (32, 1)\n",
    "    ])\n",
    "])\n",
    "\n",
    "\n",
    "ACTIVATIONS = ('relu', 'relu6',)\n",
    "\n",
    "\n",
    "class MobileNetV1(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 config:     str,\n",
    "                 capacity:   float = 1.0,\n",
    "                 activation: str = 'ReLU',\n",
    "                 n_classes:  int = 1000,\n",
    "                 seed:       int = -1):\n",
    "\n",
    "        # validate inputs\n",
    "        config = config.upper()  # canonicalise\n",
    "        if config not in CONFIGS.keys():\n",
    "            raise ValueError  # invalid configuration\n",
    "            \n",
    "        if not (0.0 < capacity <= 1.0):\n",
    "            raise ValueError  # capacity must be a positive, compressive (i.e., not greater than one) scaling factor\n",
    "\n",
    "        activation = activation.lower()  # canonicalise\n",
    "        if activation not in ACTIVATIONS:\n",
    "            raise ValueError  # invalid activation function\n",
    "        if activation == 'relu':\n",
    "            activation_class = nn.ReLU\n",
    "        else:  # activation == 'relu6':\n",
    "            activation_class = nn.ReLU6\n",
    "\n",
    "        super(MobileNetV1, self).__init__()\n",
    "\n",
    "        # build the network\n",
    "        base_width      = int(32 * capacity)\n",
    "        self.pilot      = MobileNetV1.make_pilot(base_width, activation_class)\n",
    "        self.features   = MobileNetV1.make_features(config, base_width, activation_class)\n",
    "        self.avgpool    = MobileNetV1.make_avgpool()\n",
    "        self.classifier = MobileNetV1.make_classifier(config, base_width, n_classes)\n",
    "\n",
    "        self._initialize_weights(seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_standard_convolution_layer(in_channels:      int,\n",
    "                                        out_channels:     int,\n",
    "                                        stride:           Union[int, Tuple[int, ...]],\n",
    "                                        activation_class: type) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        modules += [nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=stride, padding=(1, 1), bias=False)]\n",
    "        modules += [nn.BatchNorm2d(out_channels)]\n",
    "        modules += [activation_class(inplace=True)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_depthwise_separable_convolution_block(in_channels:      int,\n",
    "                                                   out_channels:     int,\n",
    "                                                   stride:           Union[int, Tuple[int, ...]],\n",
    "                                                   activation_class: type) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        # depthwise\n",
    "        modules += [nn.Conv2d(in_channels, in_channels, kernel_size=(3, 3), stride=stride, padding=(1, 1), groups=in_channels, bias=False)]\n",
    "        modules += [nn.BatchNorm2d(in_channels)]\n",
    "        modules += [activation_class(inplace=True)]\n",
    "\n",
    "        # pointwise\n",
    "        modules += [nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False)]\n",
    "        modules += [nn.BatchNorm2d(out_channels)]\n",
    "        modules += [activation_class(inplace=True)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_pilot(base_width:       int,\n",
    "                   activation_class: type) -> nn.Sequential:\n",
    "\n",
    "        in_channels = 3\n",
    "        out_channels = base_width\n",
    "\n",
    "        return MobileNetV1.make_standard_convolution_layer(in_channels=in_channels,\n",
    "                                                           out_channels=out_channels,\n",
    "                                                           stride=2,  # we start with a spatial down-sampling\n",
    "                                                           activation_class=activation_class)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_features(config:           str,\n",
    "                      base_width:       int,\n",
    "                      activation_class: type) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        in_channels = base_width\n",
    "        for n_channels_multiplier, stride in CONFIGS[config]:\n",
    "            out_channels = base_width * n_channels_multiplier\n",
    "            modules += [MobileNetV1.make_depthwise_separable_convolution_block(in_channels=in_channels,\n",
    "                                                                               out_channels=out_channels,\n",
    "                                                                               stride=stride,\n",
    "                                                                               activation_class=activation_class)]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_avgpool() -> nn.AdaptiveAvgPool2d:\n",
    "        return nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def make_classifier(config:     str,\n",
    "                        base_width: int,\n",
    "                        n_classes:  int) -> nn.Linear:\n",
    "\n",
    "        last_n_channels_multiplier = CONFIGS[config][-1][0]\n",
    "        in_channels = last_n_channels_multiplier * base_width\n",
    "        in_features = in_channels * 1 * 1\n",
    "\n",
    "        return nn.Linear(in_features=in_features, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pilot(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, seed: int = -1):\n",
    "\n",
    "        if seed >= 0:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db8ff7",
   "metadata": {},
   "source": [
    "We are ready to create our MobileNetV1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13771ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a MobileNetV1\n",
    "config = 'standard'\n",
    "capacity = 0.75\n",
    "activation = 'relu'  # other option: `relu6`\n",
    "\n",
    "mnv1 = MobileNetV1(config=config, capacity=capacity, activation=activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852084f",
   "metadata": {},
   "source": [
    "PyTorch networks are modelled as hierarchies of `Module`s.\n",
    "QuantLib provides a useful `lightweight` sub-package to traverse these hierarchies and provide an overview into its atoms (i.e., non-container `Module`s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ba8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      0.pilot\tConv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      1.pilot\tBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      2.pilot\tReLU(inplace=True)\n",
      " 0.0.features\tConv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      " 1.0.features\tBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.0.features\tReLU(inplace=True)\n",
      " 3.0.features\tConv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.0.features\tBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.0.features\tReLU(inplace=True)\n",
      " 0.1.features\tConv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      " 1.1.features\tBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.1.features\tReLU(inplace=True)\n",
      " 3.1.features\tConv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.1.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.1.features\tReLU(inplace=True)\n",
      " 0.2.features\tConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      " 1.2.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.2.features\tReLU(inplace=True)\n",
      " 3.2.features\tConv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.2.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.2.features\tReLU(inplace=True)\n",
      " 0.3.features\tConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      " 1.3.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.3.features\tReLU(inplace=True)\n",
      " 3.3.features\tConv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.3.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.3.features\tReLU(inplace=True)\n",
      " 0.4.features\tConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      " 1.4.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.4.features\tReLU(inplace=True)\n",
      " 3.4.features\tConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.4.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.4.features\tReLU(inplace=True)\n",
      " 0.5.features\tConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      " 1.5.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.5.features\tReLU(inplace=True)\n",
      " 3.5.features\tConv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.5.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.5.features\tReLU(inplace=True)\n",
      " 0.6.features\tConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.6.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.6.features\tReLU(inplace=True)\n",
      " 3.6.features\tConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.6.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.6.features\tReLU(inplace=True)\n",
      " 0.7.features\tConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.7.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.7.features\tReLU(inplace=True)\n",
      " 3.7.features\tConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.7.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.7.features\tReLU(inplace=True)\n",
      " 0.8.features\tConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.8.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.8.features\tReLU(inplace=True)\n",
      " 3.8.features\tConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.8.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.8.features\tReLU(inplace=True)\n",
      " 0.9.features\tConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.9.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.9.features\tReLU(inplace=True)\n",
      " 3.9.features\tConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.9.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.9.features\tReLU(inplace=True)\n",
      "0.10.features\tConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "1.10.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2.10.features\tReLU(inplace=True)\n",
      "3.10.features\tConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4.10.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5.10.features\tReLU(inplace=True)\n",
      "0.11.features\tConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "1.11.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2.11.features\tReLU(inplace=True)\n",
      "3.11.features\tConv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4.11.features\tBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5.11.features\tReLU(inplace=True)\n",
      "0.12.features\tConv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "1.12.features\tBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2.12.features\tReLU(inplace=True)\n",
      "3.12.features\tConv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4.12.features\tBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5.12.features\tReLU(inplace=True)\n",
      "      avgpool\tAdaptiveAvgPool2d(output_size=(1, 1))\n",
      "   classifier\tLinear(in_features=768, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import quantlib.editing.graphs as qg\n",
    "\n",
    "mnv1_lw = qg.lw.quantlib_traverse(mnv1)\n",
    "mnv1_lw.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd8689",
   "metadata": {},
   "source": [
    "A fundamental functionality of PyTorch is its convenient interface to use GPUs.\n",
    "Before we explain how this interface works, we need to recap how PyTorch `Tensor`s work.\n",
    "\n",
    "Each PyTorch `Tensor` is a wrapper structure around a *payload* array.\n",
    "Apart from this payload, a `Tensor` has other attributes and methods, most of which operate on the payload (we call these *payload methods*).\n",
    "One of these attributes is `device`, which indicates whether the payload is stored on the main memory of the computing system (i.e., the one managed directly by the CPU) or on the memory of some GPU attached to the computing system.\n",
    "\n",
    "The C++ backend of `Tensor`s includes several implementations of payload methods, one for each computing device (CPU vs. GPU) that might be available on our computing system.\n",
    "At runtime, depending on the `device` attributes of the `Tensor`s involved in a given operation, PyTorch's engine dispatches the code to the correct version of the payload method.\n",
    "\n",
    "Most `Module`s have parameters or hyper-parameters, but moving them individually and manually to the correct device memory can become cumbersome.\n",
    "Thus, PyTorch `Module`s expose a `to` method which, if passed the `device` argument, automatically moves all the parameters and hyper-parameters to the correct device memory.\n",
    "\n",
    "If more than one GPU is available on our system, it is possible to wrap a `Module` into an `nn.DataParallel` object, which will automatically map model replicas to all the GPUs and partition mini-batches to exploit all the available computational power.\n",
    "However, we note that most of QuantLib's editing functionalities only work on `Module`s.\n",
    "For this reason, we prefer to keep at least one symbolic handle to the main `Module` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34fa5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_migrate_to_gpu(network: nn.Module,\n",
    "                         device:  torch.device,\n",
    "                         n_gpus:  int) -> Tuple[nn.Module, Union[nn.Module, nn.DataParallel]]:\n",
    "    \"\"\"If GPUs are avaiable, migrate the network there for better time performance.\"\"\"\n",
    "\n",
    "    if n_gpus > 0:\n",
    "        network = network.to(device=device)  # move the model parameters to the lead GPU\n",
    "    \n",
    "    if n_gpus > 1:\n",
    "        maybenndp_network = nn.DataParallel(network)  # at runtime, the model will be replicated on each available GPU\n",
    "    else:\n",
    "        maybenndp_network = network\n",
    "    \n",
    "    return network, maybenndp_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6923c",
   "metadata": {},
   "source": [
    "Let's map our MobileNetV1 to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8623678",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnv1, maybenndp_mnv1 = maybe_migrate_to_gpu(mnv1, device, n_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32beefcf",
   "metadata": {},
   "source": [
    "### Step 4: evaluate the performance of a raw network\n",
    "\n",
    "As in most experimental sciences, also in machine learning it is crucial to perform elementary experiments.\n",
    "These experiments are important to:\n",
    "* validate assumptions that, if violated, could invalidate all the following experiments;\n",
    "* establish baselines against which we can compare future results.\n",
    "\n",
    "Given that in the ImageNet validation set the 1000 classes are balancedly represented, our assumption is that the accuracy of an untrained MobileNetV1 should be around 0.1%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b4558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label(NamedTuple):\n",
    "    true:      int\n",
    "    predicted: int\n",
    "\n",
    "\n",
    "class Evaluation(OrderedDict):\n",
    "\n",
    "    def __setitem__(self, input_id: int, label: Label):\n",
    "        if not isinstance(input_id, int):\n",
    "            raise TypeError\n",
    "        if not isinstance(label, Label):\n",
    "            raise TypeError\n",
    "\n",
    "        super(Evaluation, self).__setitem__(input_id, label)\n",
    "\n",
    "    @property\n",
    "    def correct(self) -> int:\n",
    "        return sum((label.true == label.predicted) for label in self.values())\n",
    "\n",
    "    @property\n",
    "    def accuracy(self) -> float:\n",
    "        return 100.0 * (float(self.correct) / len(self))\n",
    "\n",
    "    def compare(self, other: Evaluation) -> float:\n",
    "        \"\"\"Return the percentage of matching predictions.\"\"\"\n",
    "\n",
    "        if len(set(self.keys()).symmetric_difference(set(other.keys()))) > 0:\n",
    "            raise ValueError  # can only compare evaluations carried out on the same data points\n",
    "\n",
    "        # else, I proceed with the comparison\n",
    "        matched: int = 0\n",
    "        for input_id, label in self.items():\n",
    "            other_label = other[input_id]\n",
    "            if label.predicted == other_label.predicted:\n",
    "                matched += 1\n",
    "\n",
    "        return 100.0 * (float(matched) / len(self))\n",
    "\n",
    "    \n",
    "def evaluate_network(loader:  torch.utils.data.DataLoader,\n",
    "                     network: Union[nn.Module, nn.DataParallel],\n",
    "                     device:  torch.device) -> Evaluation:\n",
    "\n",
    "    if not isinstance(loader.sampler, torch.utils.data.SequentialSampler):\n",
    "        raise ValueError  # the order of the data points is not deterministic, and the input IDs lose their meaning\n",
    "    \n",
    "    evaluation = Evaluation()\n",
    "    base_input_id: int = 0\n",
    "\n",
    "    for x, y_true in loader:\n",
    "        \n",
    "        x = x.to(device=device)\n",
    "        y_true = y_true.to(device=device)\n",
    "\n",
    "        y_pred = torch.argmax(network(x), dim=1)\n",
    "        \n",
    "        for i, (yt, yp) in enumerate(zip(y_true.flatten(), y_pred.flatten())):\n",
    "            evaluation[base_input_id + i] = Label(int(yt), int(yp))\n",
    "        base_input_id += len(x)\n",
    "        \n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02d4b2",
   "metadata": {},
   "source": [
    "Let's evaluate the network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c8c22b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (floating-point, untrained):   0.10%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv1.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv1_perf = evaluate_network(valid_loader, maybenndp_mnv1, device)\n",
    "print(\"Accuracy (floating-point, untrained): {:6.2f}%.\".format(mnv1_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv1.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77bd6f",
   "metadata": {},
   "source": [
    "The accuracy is in line with our expecations: a positive sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3829ac3",
   "metadata": {},
   "source": [
    "## Part 2: training and evaluating a fake-quantised MobileNetV1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971e73a",
   "metadata": {},
   "source": [
    "### Step 1: retrieve a pre-trained floating-point network\n",
    "\n",
    "In some cases, quantisation algorithms can be applied to pre-trained floating-point networks.\n",
    "These algorithms can be classified in:\n",
    "* **post-training quantisation (PTQ)** algorithms, which do not need to run any gradient descent iteration or apply any parameter updates;\n",
    "* **quantisation-aware fine-tuning (QAFT)** algorithms, applications of *quantisation-aware training (QAT)* algorithms that last at most a few epochs.\n",
    "\n",
    "To speed up our work, we create a new MobileNetV1 and load such a pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d77675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a MobileNetV1\n",
    "config = 'standard'\n",
    "capacity = 0.75\n",
    "activation = 'relu'  # other option: `relu6`\n",
    "\n",
    "mnv1 = MobileNetV1(config=config, capacity=capacity, activation=activation)\n",
    "mnv1, maybenndp_mnv1 = maybe_migrate_to_gpu(mnv1, device, n_gpus)\n",
    "\n",
    "# get the path to the floating-point checkpoint\n",
    "path_logs = os.path.join(os.curdir, 'logs')\n",
    "fp_checkpoint_filename = '_'.join(['MNv1', str(capacity), activation]) + '.ckpt'\n",
    "path_fp_checkpoint = os.path.join(path_logs, fp_checkpoint_filename)\n",
    "if not os.path.isfile(path_fp_checkpoint):\n",
    "    raise FileNotFoundError\n",
    "\n",
    "# load the pre-trained parameters into the network object\n",
    "pretrained_state_dict = torch.load(path_fp_checkpoint)\n",
    "mnv1.load_state_dict(pretrained_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fee07",
   "metadata": {},
   "source": [
    "Let's evaluate the network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eaeb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (floating-point, trained):  68.71%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv1.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv1_perf = evaluate_network(valid_loader, maybenndp_mnv1, device)\n",
    "print(\"Accuracy (floating-point, trained): {:6.2f}%.\".format(mnv1_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv1.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938614e0",
   "metadata": {},
   "source": [
    "### Step 2: perform the float-to-fake (F2F) conversion\n",
    "\n",
    "QuantLib's `editing` package implements the building blocks of a rudimental compiler to quantise `nn.Module`s and transform them into quantised neural networks.\n",
    "This package consists of two sub-packages:\n",
    "* `graphs`, extending PyTorch's `nn` and `fx` namespaces;\n",
    "* `editing`, implementing the computational graph annotation and rewriting functionalities.\n",
    "\n",
    "The first step towards the quantisation of our MobileNetV1 is replacing its composing `nn.Module`s with counterparts that support quantisation.\n",
    "In QuantLib, these counterparts are `_QModule`s.\n",
    "\n",
    "As a first step, we need to trace our floating-point network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abbca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace\n",
    "mnv1.eval()  # remember to freeze parameters, since the `Editor`s might operate with them\n",
    "mnv1fp = qg.fx.quantlib_symbolic_trace(root=mnv1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6fdf4",
   "metadata": {},
   "source": [
    "Now that we have an `fx.GraphModule` object, we can pass it to the tool performing the so-called **float-to-fake (F2F)** conversion.\n",
    "\n",
    "In this case, we are aiming for a quantised network using signed 8-bit integers for the weight arrays, and unsigned 8-bit integers for the feature arrays.\n",
    "The quantisation algorithm is **parametrised clipping activation (PACT)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76453995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.editing as qe\n",
    "\n",
    "f2fconverter = qe.f2f.F2F8bitPACTConverter()\n",
    "mnv1fq_uninit = f2fconverter(mnv1fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f9e7b",
   "metadata": {},
   "source": [
    "Let's visually inspect whether the conversion was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19f5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      0.pilot\tPACTConv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      1.pilot\tBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      2.pilot\tPACTReLU(inplace=True)\n",
      " 0.0.features\tPACTConv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      " 1.0.features\tBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.0.features\tPACTReLU(inplace=True)\n",
      " 3.0.features\tPACTConv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.0.features\tBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.0.features\tPACTReLU(inplace=True)\n",
      " 0.1.features\tPACTConv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
      " 1.1.features\tBatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.1.features\tPACTReLU(inplace=True)\n",
      " 3.1.features\tPACTConv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.1.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.1.features\tPACTReLU(inplace=True)\n",
      " 0.2.features\tPACTConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
      " 1.2.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.2.features\tPACTReLU(inplace=True)\n",
      " 3.2.features\tPACTConv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.2.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.2.features\tPACTReLU(inplace=True)\n",
      " 0.3.features\tPACTConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      " 1.3.features\tBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.3.features\tPACTReLU(inplace=True)\n",
      " 3.3.features\tPACTConv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.3.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.3.features\tPACTReLU(inplace=True)\n",
      " 0.4.features\tPACTConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      " 1.4.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.4.features\tPACTReLU(inplace=True)\n",
      " 3.4.features\tPACTConv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.4.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.4.features\tPACTReLU(inplace=True)\n",
      " 0.5.features\tPACTConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      " 1.5.features\tBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.5.features\tPACTReLU(inplace=True)\n",
      " 3.5.features\tPACTConv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.5.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.5.features\tPACTReLU(inplace=True)\n",
      " 0.6.features\tPACTConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.6.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.6.features\tPACTReLU(inplace=True)\n",
      " 3.6.features\tPACTConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.6.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.6.features\tPACTReLU(inplace=True)\n",
      " 0.7.features\tPACTConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.7.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.7.features\tPACTReLU(inplace=True)\n",
      " 3.7.features\tPACTConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.7.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.7.features\tPACTReLU(inplace=True)\n",
      " 0.8.features\tPACTConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.8.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.8.features\tPACTReLU(inplace=True)\n",
      " 3.8.features\tPACTConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.8.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.8.features\tPACTReLU(inplace=True)\n",
      " 0.9.features\tPACTConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      " 1.9.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 2.9.features\tPACTReLU(inplace=True)\n",
      " 3.9.features\tPACTConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      " 4.9.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      " 5.9.features\tPACTReLU(inplace=True)\n",
      "0.10.features\tPACTConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "1.10.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2.10.features\tPACTReLU(inplace=True)\n",
      "3.10.features\tPACTConv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4.10.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5.10.features\tPACTReLU(inplace=True)\n",
      "0.11.features\tPACTConv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "1.11.features\tBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2.11.features\tPACTReLU(inplace=True)\n",
      "3.11.features\tPACTConv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4.11.features\tBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5.11.features\tPACTReLU(inplace=True)\n",
      "0.12.features\tPACTConv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "1.12.features\tBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "2.12.features\tPACTReLU(inplace=True)\n",
      "3.12.features\tPACTConv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4.12.features\tBatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "5.12.features\tPACTReLU(inplace=True)\n",
      "      avgpool\tAdaptiveAvgPool2d(output_size=(1, 1))\n",
      "   classifier\tPACTLinear(in_features=768, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "mnv1fq_uninit_lw = qg.lw.quantlib_traverse(root=mnv1fq_uninit)\n",
    "mnv1fq_uninit_lw.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5b676",
   "metadata": {},
   "source": [
    "Now we are going to prepare all the ingredients to train and validate our fake-quantised network:\n",
    "* the loss function; in PyTorch, loss functions are implemented as `nn.Module`s;\n",
    "* the optimiser; in PyTorch, optimisers are implemented as `torch.optim.Optimizer` objects, and their responsibility is updating the parameters, **not** performing gradient descent (which is a prerogative of the *autograd engine*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b721364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPUs are available, migrate the network\n",
    "mnv1fq_uninit, maybenndp_mnv1fq_uninit = maybe_migrate_to_gpu(mnv1fq_uninit, device, n_gpus)\n",
    "\n",
    "# create the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# create the optimiser; since we use PACT, we need an `Optimizer` capable of updating the clipping bounds independently of other parameters\n",
    "import quantlib.algorithms as qa\n",
    "optimiser = qa.qalgorithms.qatalgorithms.pact.PACTSGD(mnv1fq_uninit, pact_decay=0.001, lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44ec45",
   "metadata": {},
   "source": [
    "Multiple iterations through the training data set might be required to bring the model to convergence.\n",
    "Since for each training iteration we also want to validate the performance of our model on the validation set, we define convenient functions to run individual training and validation epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4878c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader:    torch.utils.data.DataLoader,\n",
    "                    network:   Union[nn.Module, nn.DataParallel],\n",
    "                    device:    torch.device,\n",
    "                    loss_fn:   nn.Module,\n",
    "                    optimiser: torch.optim.Optimiser,\n",
    "                    verbose:   bool = False):\n",
    "    \n",
    "    network.train()\n",
    "\n",
    "    # statistical performance counters  # TODO: define a `StatisticalPerformanceCounters` object\n",
    "    n_points:   int = 0\n",
    "    correct:    int = 0\n",
    "    total_loss: float = 0.0\n",
    "    \n",
    "    for batch_id, (x, y_true) in enumerate(loader):\n",
    "        \n",
    "        # cast data points to the network's device\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = network(x)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        \n",
    "        # update performance counters\n",
    "        n_points += len(x)\n",
    "        correct += int(torch.sum(y_true == y_pred.argmax(dim=1)))\n",
    "        total_loss = total_loss + (loss.item() * len(x))\n",
    "        if verbose:\n",
    "            print(\"Training batch [{:5d}/{:5d}] - Loss: {:8.3f} - Accuracy: {:6.2f}%\".format(batch_id, len(loader), total_loss / n_points, 100.0 * (float(correct) / n_points)))\n",
    "        \n",
    "        # backward pass\n",
    "        optimiser.zero_grad()  # clear old gradients\n",
    "        loss.backward()        # compute new gradients\n",
    "        optimiser.step()       # apply gradient descent step\n",
    "\n",
    "        \n",
    "def validate_one_epoch(loader:    torch.utils.data.DataLoader,\n",
    "                       network:   Union[nn.Module, nn.DataParallel],\n",
    "                       device:    torch.device,\n",
    "                       loss_fn:   nn.Module,\n",
    "                       verbose:   bool = False):\n",
    "    \n",
    "    network.eval()\n",
    "\n",
    "    # statistical performance counters  # TODO: define a `StatisticalPerformanceCounters` object\n",
    "    n_points:   int = 0\n",
    "    correct:    int = 0\n",
    "    total_loss: float = 0.0\n",
    "    \n",
    "    for batch_id, (x, y_true) in enumerate(loader):\n",
    "        \n",
    "        # cast data points to the network's device\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = network(x)\n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "        # update performance counters\n",
    "        n_points += len(x)\n",
    "        correct += int(torch.sum(y_true == y_pred.argmax(dim=1)))\n",
    "        total_loss = total_loss + (loss.item() * len(x))\n",
    "        if verbose:\n",
    "            print(\"Validation batch [{:5d}/{:5d}] - Loss: {:8.3f} - Accuracy: {:6.2f}%\".format(batch_id, len(loader), total_loss / n_points, 100.0 * (float(correct) / n_points)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0126220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we run this \"observation\" on CPU or a single GPU, we need to limit the batch size\n",
    "warmup_valid_loader = loader_factory.get_dataloader('valid', valid_transform, 128, num_workers=n_cpus)\n",
    "\n",
    "# set validation state\n",
    "mnv1fq_uninit.eval()\n",
    "\n",
    "# collect statistics about the floating-point `Tensor`s passing through the quantisers, so that we can better fit the quantisers' hyper-parameters\n",
    "# start observing\n",
    "for m in mnv1fq_uninit.modules():\n",
    "    if isinstance(m, tuple(qa.qalgorithms.qatalgorithms.pact.NNMODULE_TO_PACTMODULE.values())):\n",
    "        m.start_observing()\n",
    "# collect statistics\n",
    "validate_one_epoch(warmup_valid_loader, mnv1fq_uninit, device, loss_fn)\n",
    "# stop observing\n",
    "for m in mnv1fq_uninit.modules():\n",
    "    if isinstance(m, tuple(qa.qalgorithms.qatalgorithms.pact.NNMODULE_TO_PACTMODULE.values())):\n",
    "        m.stop_observing()\n",
    "\n",
    "# restore training state\n",
    "mnv1fq_uninit.train()\n",
    "        \n",
    "mnv1fq_init, maybenndp_mnv1fq_init = mnv1fq_uninit, maybenndp_mnv1fq_uninit  # now the quantisers' hyper-parameters are initialised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e4b6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (fake-quantised, untrained):   0.21%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv1fq_init.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv1fq_perf = evaluate_network(valid_loader, maybenndp_mnv1fq_init, device)\n",
    "print(\"Accuracy (fake-quantised, untrained): {:6.2f}%.\".format(mnv1fq_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv1fq_init.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "042ae5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_checkpoint_base_filename, extension = fp_checkpoint_filename.rsplit('.', 1)\n",
    "fq_checkpoint_base_filename = '_'.join([fp_checkpoint_base_filename, 'FQ', 'uint8x', 'int8w'])\n",
    "fq_checkpoint_filename = '.'.join([fq_checkpoint_base_filename, extension])\n",
    "\n",
    "path_fq_checkpoint = os.path.join(path_logs, fq_checkpoint_filename)\n",
    "\n",
    "if not os.path.isfile(path_fq_checkpoint):\n",
    "    train_one_epoch(train_loader, maybenndp_mnv1fq_init, device, loss_fn, optimiser, verbose=True)\n",
    "    torch.save(mnv1fq_init.state_dict(), ckpt_filename)\n",
    "\n",
    "mnv1fq_init.load_state_dict(torch.load(path_fq_checkpoint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60121430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (fake-quantised, trained):  68.34%.\n"
     ]
    }
   ],
   "source": [
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "maybenndp_mnv1fq_init.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv1fq_init_perf = evaluate_network(valid_loader, maybenndp_mnv1fq_init, device)\n",
    "print(\"Accuracy (fake-quantised, trained): {:6.2f}%.\".format(mnv1fq_init_perf.accuracy))\n",
    "\n",
    "# restore the training mode\n",
    "maybenndp_mnv1fq_init.train()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f7c1b",
   "metadata": {},
   "source": [
    "We now have a fake-quantised model with approximately the same accuracy as the floating-point one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb286627",
   "metadata": {},
   "source": [
    "## Part 3: integerising a fake-quantised network\n",
    "\n",
    "A trained fake-quantised network is not a true integerised program.\n",
    "To obtain such a network, we apply the so-called **fake-to-true (F2T)** conversion, a sequence of program transformations to turn a fake-quantised network into an integerised one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6ba3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNv1Head(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MNv1Head, self).__init__()\n",
    "        self.eps = qg.nn.EpsTunnel(torch.Tensor([1.0]))\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.lin = nn.Linear(1, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.eps(x)\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), 1)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MNv1HeadApplier(qe.editors.nnmodules.NNModuleApplier):\n",
    "\n",
    "    def __init__(self, rn18headpattern: qe.editors.nnmodules.GenericNNModulePattern):\n",
    "        super(MNv1HeadApplier, self).__init__(rn18headpattern)\n",
    "\n",
    "    def _apply(self, g: fx.GraphModule, ap: qe.editors.nnmodules.NodesMap, id_: str) -> fx.GraphModule:\n",
    "\n",
    "        name_to_match_node = self.pattern.name_to_match_node(nodes_map=ap)\n",
    "        node_lin = name_to_match_node['lin']\n",
    "\n",
    "        name_to_match_module = self.pattern.name_to_match_module(nodes_map=ap, data_gm=g)\n",
    "        module_eps = name_to_match_module['eps']\n",
    "        module_lin = name_to_match_module['lin']\n",
    "\n",
    "        assert module_eps.eps_out.numel() == 1\n",
    "        assert len(node_lin.all_input_nodes) == 1\n",
    "\n",
    "        # create the new module\n",
    "        new_target = id_\n",
    "        new_module = nn.Linear(in_features=module_lin.in_features, out_features=module_lin.out_features, bias=module_lin.bias is not None)\n",
    "        new_weight = module_lin.weight.data.detach().clone() * module_eps.eps_out\n",
    "        new_module.weight.data = new_weight\n",
    "        if module_lin.bias is not None:\n",
    "            new_bias = module_lin.bias.data.detach().clone()\n",
    "            new_module.bias.data = new_bias\n",
    "\n",
    "        # add the requantised linear operation to the graph...\n",
    "        g.add_submodule(new_target, new_module)\n",
    "        linear_input = next(iter(node_lin.all_input_nodes))\n",
    "        with g.graph.inserting_after(linear_input):\n",
    "            new_node = g.graph.call_module(new_target, args=(linear_input,))\n",
    "        node_lin.replace_all_uses_with(new_node)\n",
    "\n",
    "        module_eps.set_eps_out(torch.ones_like(module_eps.eps_out))\n",
    "\n",
    "        # ...and delete the old operation\n",
    "        g.delete_submodule(node_lin.target)\n",
    "        g.graph.erase_node(node_lin)\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "class MNv1HeadRewriter(qe.editors.nnmodules.NNModuleRewriter):\n",
    "\n",
    "    def __init__(self):\n",
    "        # create pattern\n",
    "        rn18headwithcheckers = qe.editors.nnmodules.NNModuleWithCheckers(MNv1Head(), {})\n",
    "        rn18headpattern = qe.editors.nnmodules.GenericNNModulePattern(qg.fx.quantlib_symbolic_trace, rn18headwithcheckers)\n",
    "        # create matcher and applier\n",
    "        finder = qe.editors.nnmodules.GenericGraphMatcher(rn18headpattern)\n",
    "        applier = MNv1HeadApplier(rn18headpattern)\n",
    "        # link pattern, matcher, and applier into the rewriter\n",
    "        super(MNv1HeadRewriter, self).__init__('MNv1HeadRewriter', rn18headpattern, finder, applier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faa52eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2T conversion and ONNX exporting require structural information about the input\n",
    "x, _ = next(iter(valid_loader))\n",
    "x = x[0].unsqueeze(0)\n",
    "\n",
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "mnv1fq_init = mnv1fq_init.to(torch.device('cpu'))  # TODO: the `Tensor`s generated inside the F2T conversion flow are generated for CPU\n",
    "mnv1fq_init.eval()\n",
    "\n",
    "# perform the conversion\n",
    "f2tconverter = qe.f2t.F2T24bitConverter(custom_editor=MNv1HeadRewriter())\n",
    "mnv1tq = f2tconverter(mnv1fq_init, {'x': {'shape': x.shape, 'scale': torch.Tensor([ImageNetStats['quantise']['scale']])}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f813076",
   "metadata": {},
   "source": [
    "To validate the performance of the integerised network, we need to pass it integerised data.\n",
    "Thus, we create a `DataLoader` yielding `Tensor` images with integer components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bcfe95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the validation `DataLoader` returning integerised (UINT8) images\n",
    "int_valid_transform = ImageNetTransform('valid', integerise=True)\n",
    "int_valid_loader = loader_factory.get_dataloader('valid', int_valid_transform, 128, num_workers=n_cpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41824c69",
   "metadata": {},
   "source": [
    "Let's evaluate our true-quantised MobileNetV1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d20c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (true-quantised, trained):  68.16%.\n"
     ]
    }
   ],
   "source": [
    "mnv1tq = mnv1tq.to(device=device)\n",
    "\n",
    "# set the network in evaluation mode to \"freeze\" the parameters of batch normalisations\n",
    "mnv1tq.eval()\n",
    "\n",
    "# evaluate the network on the validation set\n",
    "mnv1tq_perf = evaluate_network(int_valid_loader, mnv1tq, device)\n",
    "print(\"Accuracy (true-quantised, trained): {:6.2f}%.\".format(mnv1tq_perf.accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fce75c",
   "metadata": {},
   "source": [
    "We notice a minor 0.2% drop in accuracy with respect to the fake-quantised network.\n",
    "Likely, this dicrepancy is due to the propagation of small numerical differences between floating-point batch-normalisations and integerised requantisations.\n",
    "However, the performance of the network is still very close to the original floating-point one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e8553",
   "metadata": {},
   "source": [
    "## Part 4: export a backend-specific ONNX model\n",
    "\n",
    "The QuantLib `backends` package contains the abstractions required to export **ONNX** models, and possibly annotate them with backend-specific information.\n",
    "To demonstrate its usage, we consider the [DORY](https://github.com/pulp-platform/dory) backend.\n",
    "\n",
    "First, we create the folder to host DORY-specific files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7827655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "backend_name = 'DORY'\n",
    "path_export = os.path.join(os.curdir, backend_name)\n",
    "\n",
    "if os.path.isdir(path_export):  # remove old files\n",
    "    shutil.rmtree(path_export)\n",
    "os.mkdir(path_export)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1ed8f",
   "metadata": {},
   "source": [
    "Then, we export an annotated ONNX model.\n",
    "DORY can also verify whether the exported ONNX model can be correctly transformed into an integerised program for PULP platforms.\n",
    "To perform this consistency checks, we need to dump the input and features maps corresponding to an example data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2ec9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.backends as qb\n",
    "\n",
    "x = x.to(torch.device('cpu'))\n",
    "mnv1tq = mnv1tq.to(device=torch.device('cpu'))\n",
    "\n",
    "exporter = qb.dory.DORYExporter()\n",
    "exporter.export(network=mnv1tq, input_shape=x.shape, path=path_export)\n",
    "exporter.dump_features(network=mnv1tq, x=x, path=path_export)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7fed0",
   "metadata": {},
   "source": [
    "We can use [Netron](https://netron.app/) to visually inspect that the exported model is an integerised program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a363f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have reached the bottom of the part of the deep learning stack covered by QuantLib.\n",
    "If you are interested in graph optimisations and code generation, you can read the [DORY paper](https://ieeexplore.ieee.org/document/9381618) and check out the DORY repository.\n",
    "\n",
    "Cheers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90aa843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
